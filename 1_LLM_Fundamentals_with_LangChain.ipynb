{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7347c1-3047-4d83-a0d9-993f05c3726b",
   "metadata": {},
   "source": [
    "# Learning LangChain\n",
    "by Mayo Oshin, Nuno Campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1078e4e4-0adc-4a8b-a534-9ebb1877e11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0faa1354-89ae-43c1-bbf4-23716de587b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_LLM_Fundamentals_with_LangChain.ipynb  Arquitecturas.ipynb  memory.ipynb\n",
      "Agentes_I.ipynb                          Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6bda04-3734-4a5c-8e81-3873d0d8e56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ir al directorio principal\n",
    "from os import chdir\n",
    "\n",
    "# chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b77fb5-d530-4b19-b586-55ae93b1fed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'learning_langgraph': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls learning_langgraph -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58da26b0-5657-4abd-9df8-c6536936c6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargar varibles\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58015001-255f-4607-8c60-d11f89fe9e5a",
   "metadata": {},
   "source": [
    "\n",
    "# **Capítulo 1. Fundamentos de LLM con LangChain**\n",
    "\n",
    "El prefacio te dio una probadita del poder del *prompting* de LLM, donde vimos de primera mano el impacto que diferentes técnicas de *prompting* pueden tener en lo que obtienes de los LLM, especialmente cuando se combinan con criterio. El desafío al construir buenas aplicaciones de LLM radica, de hecho, en cómo construir eficazmente el *prompt* enviado al modelo y procesar la predicción del modelo para devolver una salida precisa (ver Figura 1-1).\n",
    "\n",
    "**Figura 1-1. El desafío de hacer que los LLM sean una parte útil de tu aplicación**\n",
    "\n",
    "Si puedes resolver este problema, estás en buen camino para construir aplicaciones de LLM, tanto simples como complejas. En este capítulo, aprenderás más sobre cómo los bloques de construcción de LangChain se mapean a los conceptos de LLM y cómo, cuando se combinan eficazmente, te permiten construir aplicaciones de LLM. Pero primero, la barra lateral \"¿Por qué LangChain?\" es una breve introducción de por qué creemos que es útil usar LangChain para construir aplicaciones de LLM.\n",
    "\n",
    "**¿POR QUÉ LANGCHAIN?**\n",
    "\n",
    "Por supuesto, puedes construir aplicaciones de LLM sin LangChain. La alternativa más obvia es usar el kit de desarrollo de software (SDK) —el paquete que expone los métodos de su API HTTP como funciones en el lenguaje de programación de tu elección— del proveedor de LLM que probaste primero (por ejemplo, OpenAI). Creemos que aprender LangChain valdrá la pena a corto y largo plazo debido a los siguientes factores:\n",
    "\n",
    "**Patrones comunes preconstruidos**\n",
    "\n",
    "LangChain viene con implementaciones de referencia de los patrones de aplicación de LLM más comunes (mencionamos algunos de estos en el prefacio: cadena de pensamiento, llamada a herramientas y otros). Esta es la forma más rápida de comenzar con los LLM y, a menudo, podría ser todo lo que necesitas. Sugerimos comenzar cualquier aplicación nueva a partir de estos y verificar si los resultados listos para usar son lo suficientemente buenos para tu caso de uso. Si no, consulta el siguiente punto para la otra mitad de las bibliotecas de LangChain.\n",
    "\n",
    "**Bloques de construcción intercambiables**\n",
    "\n",
    "Estos son componentes que se pueden intercambiar fácilmente por alternativas. Cada componente (un LLM, un modelo de chat, un analizador de salida, etc.—más sobre esto en breve) sigue una especificación compartida, lo que hace que tu aplicación esté preparada para el futuro. A medida que los proveedores de modelos lanzan nuevas capacidades y a medida que cambian tus necesidades, puedes hacer evolucionar tu aplicación sin tener que reescribirla cada vez.\n",
    "\n",
    "A lo largo de este libro, utilizamos los siguientes componentes principales en los ejemplos de código:\n",
    "\n",
    "* **LLM/modelo de chat:** OpenAI\n",
    "* **Embeddings:** OpenAI\n",
    "* **Almacén de vectores:** PGVector\n",
    "\n",
    "Puedes intercambiar cada uno de estos por cualquiera de las alternativas que se enumeran en las siguientes páginas:\n",
    "\n",
    "**Modelos de chat**\n",
    "\n",
    "Consulta la documentación de LangChain. Si no quieres usar OpenAI (una API comercial), te sugerimos Anthropic como una alternativa comercial u Ollama como una de código abierto.\n",
    "\n",
    "**Embeddings**\n",
    "\n",
    "Consulta la documentación de LangChain. Si no quieres usar OpenAI (una API comercial), te sugerimos Cohere como una alternativa comercial u Ollama como una de código abierto.\n",
    "\n",
    "**Almacenes de vectores**\n",
    "\n",
    "Consulta la documentación de LangChain. Si no quieres usar PGVector (una extensión de código abierto para la popular base de datos SQL Postgres), te sugerimos usar Weaviate (un almacén de vectores dedicado) u OpenSearch (características de búsqueda vectorial que forman parte de una popular base de datos de búsqueda).\n",
    "\n",
    "Este esfuerzo va más allá, por ejemplo, de que todos los LLM tengan los mismos métodos, con argumentos y valores de retorno similares. Veamos el ejemplo de los modelos de chat y dos proveedores populares de LLM, OpenAI y Anthropic. Ambos tienen una API de chat que recibe mensajes de chat (definidos vagamente como objetos con un string de tipo y un string de contenido) y devuelve un nuevo mensaje generado por el modelo. Pero si intentas usar ambos modelos en la misma conversación, inmediatamente encontrarás problemas, ya que sus formatos de mensajes de chat son sutilmente incompatibles. LangChain abstrae estas diferencias para permitir la construcción de aplicaciones que son verdaderamente independientes de un proveedor en particular. Por ejemplo, con LangChain, una conversación de chatbot en la que utilizas modelos de OpenAI y Anthropic funciona.\n",
    "\n",
    "Finalmente, a medida que desarrollas tus aplicaciones de LLM con varios de estos componentes, nos ha resultado útil contar con las capacidades de orquestación de LangChain:\n",
    "\n",
    "* Todos los componentes principales están instrumentados por el sistema de *callbacks* para la observabilidad (más sobre esto en el Capítulo 8).\n",
    "* Todos los componentes principales implementan la misma interfaz (más sobre esto hacia el final de este capítulo).\n",
    "* Las aplicaciones de LLM de larga duración se pueden interrumpir, reanudar o reintentar (más sobre esto en el Capítulo 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7c7f9-1828-4687-8384-196b0ef3004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lelc_0101 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0fe98-9a75-4c9b-8c69-1da053622deb",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# **Configuración de LangChain**\n",
    "\n",
    "Para seguir el resto del capítulo y los capítulos venideros, te recomendamos configurar LangChain en tu computadora primero.\n",
    "\n",
    "Consulta las instrucciones en el Prefacio con respecto a la creación de una cuenta de OpenAI y completa estos pasos si aún no lo has hecho. Si prefieres usar un proveedor de LLM diferente, consulta \"¿Por qué LangChain?\" para ver alternativas.\n",
    "\n",
    "Luego, dirígete a la página de Claves API en el sitio web de OpenAI (después de iniciar sesión en tu cuenta de OpenAI), crea una clave API y guárdala; la necesitarás pronto.\n",
    "\n",
    "**NOTA**\n",
    "\n",
    "En este libro, mostraremos ejemplos de código tanto en Python como en JavaScript (JS). LangChain ofrece la misma funcionalidad en ambos lenguajes, así que simplemente elige el que te resulte más cómodo y sigue los fragmentos de código correspondientes a lo largo del libro (los ejemplos de código para cada lenguaje son equivalentes).\n",
    "\n",
    "Primero, algunas instrucciones de configuración para los lectores que usan Python:\n",
    "\n",
    "Asegúrate de tener Python instalado. Consulta las instrucciones para tu sistema operativo.\n",
    "\n",
    "Instala Jupyter si deseas ejecutar los ejemplos en un entorno de *notebook*. Puedes hacerlo ejecutando `pip install notebook` en tu terminal.\n",
    "\n",
    "Instala la biblioteca LangChain ejecutando los siguientes comandos en tu terminal:\n",
    "\n",
    "```\n",
    "pip install langchain langchain-openai langchain-community\n",
    "pip install langchain-text-splitters langchain-postgres\n",
    "```\n",
    "\n",
    "Toma la clave API de OpenAI que generaste al principio de esta sección y hazla disponible en tu entorno de terminal. Puedes hacerlo ejecutando lo siguiente:\n",
    "\n",
    "```\n",
    "export OPENAI_API_KEY=tu-clave\n",
    "```\n",
    "\n",
    "No olvides reemplazar `tu-clave` con la clave API que generaste anteriormente.\n",
    "\n",
    "Abre un *notebook* de Jupyter ejecutando este comando:\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Ahora estás listo para seguir los ejemplos de código de Python.\n",
    "\n",
    "**Uso de LLM en LangChain**\n",
    "\n",
    "Para recapitular, los LLM son el motor impulsor detrás de la mayoría de las aplicaciones de IA generativa. LangChain proporciona dos interfaces simples para interactuar con cualquier proveedor de API de LLM:\n",
    "\n",
    "Modelos de chat\n",
    "\n",
    "LLM\n",
    "\n",
    "La interfaz LLM simplemente toma un *prompt* de cadena como entrada, envía la entrada al proveedor del modelo y luego devuelve la predicción del modelo como salida.\n",
    "\n",
    "Importemos el *wrapper* OpenAI LLM de LangChain para invocar una predicción del modelo utilizando un *prompt* simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c423053-5a33-4f33-9243-8cd8ed75a9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clear and sunny today.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "model.invoke(\"The sky is\").content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301431a5-a4e4-42fa-8006-6a402a5b92bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "**CONSEJO**\n",
    "\n",
    "Observa el parámetro `model` pasado a `OpenAI`. Este es el parámetro más común para configurar al usar un LLM o un modelo de chat, el modelo subyacente a utilizar, ya que la mayoría de los proveedores ofrecen varios modelos con diferentes compensaciones en capacidad y costo (generalmente los modelos más grandes son más capaces, pero también más caros y lentos). Consulta la descripción general de los modelos que ofrece OpenAI.\n",
    "\n",
    "Otros parámetros útiles para configurar incluyen los siguientes, ofrecidos por la mayoría de los proveedores:\n",
    "\n",
    "* `temperature`: Controla el algoritmo de muestreo utilizado para generar la salida. Los valores más bajos producen salidas más predecibles (por ejemplo, 0.1), mientras que los valores más altos generan resultados más creativos o inesperados (como 0.9). Diferentes tareas necesitarán diferentes valores para este parámetro. Por ejemplo, la producción de salida estructurada generalmente se beneficia de una temperatura más baja, mientras que las tareas de escritura creativa funcionan mejor con un valor más alto.\n",
    "* `max_tokens`: Limita el tamaño (y el costo) de la salida. Un valor más bajo puede hacer que el LLM deje de generar la salida antes de llegar a un final natural, por lo que puede parecer que se ha truncado.\n",
    "\n",
    "Más allá de estos, cada proveedor expone un conjunto diferente de parámetros. Te recomendamos consultar la documentación del que elijas. Por ejemplo, consulta la plataforma de OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336184ae-3234-4ba4-8d78-ce02507bc250",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Alternativamente, la interfaz de modelo de chat permite conversaciones de ida y vuelta entre el usuario y el modelo. La razón por la que es una interfaz separada es porque los proveedores populares de modelos de lenguaje como OpenAI diferencian los mensajes enviados hacia y desde el modelo en roles de *usuario*, *asistente* y *sistema* (aquí, el *rol* denota el tipo de contenido que contiene el mensaje):\n",
    "\n",
    "**Rol del sistema**  \n",
    "Se utiliza para dar instrucciones que el modelo debe seguir para responder a una pregunta del usuario.\n",
    "\n",
    "**Rol del usuario**  \n",
    "Se utiliza para la consulta del usuario y cualquier otro contenido producido por este.\n",
    "\n",
    "**Rol del asistente**  \n",
    "Se utiliza para el contenido generado por el modelo.\n",
    "\n",
    "La interfaz del modelo de chat facilita la configuración y gestión de las conversaciones en tu aplicación de chatbot con IA.  \n",
    "A continuación, se muestra un ejemplo utilizando el modelo `ChatOpenAI` de LangChain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7066eb80-84ed-453e-a9b5-ea7f3bc3bda7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNWFWydRDDAKPNqCmHABrb5Pya6jf', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1e1caa2-a2c3-430e-9077-1e53bc9361b9-0', usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configuración del modelo con parámetros personalizados\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.7,              # Controla la creatividad del modelo (0 = determinista, 1 = aleatorio)\n",
    "    model_name=\"gpt-3.5\",           # Puedes cambiarlo por \"gpt-3.5-turbo\" u otros modelos compatibles\n",
    "    max_tokens=500,               # Límite de tokens que puede generar el modelo en una respuesta\n",
    "    openai_api_key=\"tu_clave_aquí\"  # (opcional si ya lo configuraste en las variables de entorno)\n",
    ")\n",
    "\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab5290-ae42-4c9e-9537-5964e27c4288",
   "metadata": {},
   "source": [
    "### Otros parámetros útiles que puedes incluir:\n",
    "\n",
    "- `top_p`: alternativa a `temperature`, controla la \"nucleación\" de las respuestas.\n",
    "- `frequency_penalty`: penaliza la repetición de palabras o frases.\n",
    "- `presence_penalty`: incentiva hablar de nuevos temas.\n",
    "- `streaming`: si quieres respuestas en tiempo real (requiere configuración adicional).\n",
    "- `request_timeout`: para controlar cuánto tiempo esperas antes de que la petición falle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aea12b-896c-44a5-a84b-584c0bf47f97",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "En lugar de utilizar un único string como prompt, los modelos de chat hacen uso de distintos tipos de mensajes, asociados a cada uno de los roles mencionados anteriormente. Estos incluyen lo siguiente:\n",
    "\n",
    "- **HumanMessage**  \n",
    "  Un mensaje enviado desde la perspectiva del humano, con el rol de *usuario*.\n",
    "\n",
    "- **AIMessage**  \n",
    "  Un mensaje enviado desde la perspectiva de la IA, con el rol de *asistente*.\n",
    "\n",
    "- **SystemMessage**  \n",
    "  Un mensaje que define las instrucciones que la IA debe seguir, con el rol de *sistema*.\n",
    "\n",
    "- **ChatMessage**  \n",
    "  Un mensaje que permite establecer un rol personalizado de manera arbitraria.\n",
    "\n",
    "Vamos a incorporar una instrucción con `SystemMessage` en nuestro ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19053981-c829-4ff7-9baa-551e25890a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de Francia es París!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Crear instancia del modelo\n",
    "model = ChatOpenAI(\n",
    "    temperature=0.5,        # Puedes ajustar este valor para controlar la aleatoriedad\n",
    "    model_name=\"gpt-4\",     # Especifica el modelo a utilizar\n",
    "    max_tokens=100          # Límite de tokens en la respuesta\n",
    ")\n",
    "\n",
    "# Mensaje del sistema con instrucciones\n",
    "system_msg = SystemMessage(\n",
    "    '''Eres un asistente útil que responde a las preguntas con tres signos \n",
    "    de exclamación.'''\n",
    ")\n",
    "\n",
    "# Mensaje del usuario\n",
    "human_msg = HumanMessage(\"¿Cuál es la capital de Francia?\")\n",
    "\n",
    "# Ejecutar el modelo con los mensajes\n",
    "respuesta = model.invoke([system_msg, human_msg])\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(respuesta.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c91ab-7887-4964-94ea-a612135f6f7c",
   "metadata": {},
   "source": [
    "### 🧠 Salida esperada:\n",
    "\n",
    "```\n",
    "¡París!!!\n",
    "```\n",
    "\n",
    "Como puedes ver, el modelo obedeció la instrucción proporcionada en el `SystemMessage`, a pesar de que esta instrucción no estaba presente en la pregunta del usuario. Esto te permite preconfigurar el comportamiento de tu aplicación de IA para que responda de manera relativamente predecible según la entrada del usuario.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e341ec-6942-456d-9c83-c7d0d2cf79aa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 🔁 Haciendo Reutilizables los Prompts en Modelos LLM\n",
    "\n",
    "La sección anterior mostró cómo las instrucciones del prompt influyen significativamente en la salida del modelo. Los prompts ayudan al modelo a entender el contexto y generar respuestas relevantes a las consultas.\n",
    "\n",
    "A continuación, se muestra un ejemplo de un prompt detallado:\n",
    "\n",
    "```\n",
    "Responde la pregunta con base en el contexto que aparece abajo.  \n",
    "Si no se puede responder con la información proporcionada, responde con \"No lo sé\".\n",
    "\n",
    "Contexto: Los avances más recientes en PLN (Procesamiento de Lenguaje Natural) están siendo impulsados por los Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus contrapartes más pequeñas y se han vuelto invaluables para los desarrolladores que crean aplicaciones con capacidades de PLN. Los desarrolladores pueden aprovechar estos modelos mediante la librería `transformers` de Hugging Face, o utilizando las ofertas de OpenAI y Cohere a través de las librerías `openai` y `cohere`, respectivamente.\n",
    "\n",
    "Pregunta: ¿Qué proveedores ofrecen modelos LLM?\n",
    "\n",
    "Respuesta:\n",
    "```\n",
    "\n",
    "Aunque el prompt parece ser solo un string simple, el verdadero reto es decidir qué debe contener ese texto y cómo debe adaptarse dinámicamente a las entradas del usuario.\n",
    "\n",
    "Por suerte, **LangChain** proporciona interfaces de plantillas de prompt que hacen fácil construir instrucciones con entradas dinámicas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "330bc9d9-b57b-45f0-8cbc-4b8deeec4e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Crear la plantilla con marcadores dinámicos\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta con base en el contexto que aparece abajo.\n",
    "Si no se puede responder con la información proporcionada, responde con \"No lo sé\".\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805986af-d7bf-4e19-bd25-6fc1f53eb20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invocar la plantilla con valores dinámicos\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"Los avances más recientes en PLN están siendo impulsados por \n",
    "    Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus \n",
    "    contrapartes más pequeñas y se han vuelto invaluables para los desarrolladores \n",
    "    que crean aplicaciones con capacidades de lenguaje natural. Los desarrolladores \n",
    "    pueden aprovechar estos modelos mediante la librería `transformers` de Hugging Face \n",
    "    o utilizando las ofertas de OpenAI y Cohere a través de las librerías `openai` y `cohere`.\"\"\",\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f4cc5a-7c48-4b8f-9ace-f8775bdb5533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responde la pregunta con base en el contexto que aparece abajo.\n",
      "Si no se puede responder con la información proporcionada, responde con \"No lo sé\".\n",
      "\n",
      "Contexto: Los avances más recientes en PLN están siendo impulsados por \n",
      "    Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus \n",
      "    contrapartes más pequeñas y se han vuelto invaluables para los desarrolladores \n",
      "    que crean aplicaciones con capacidades de lenguaje natural. Los desarrolladores \n",
      "    pueden aprovechar estos modelos mediante la librería `transformers` de Hugging Face \n",
      "    o utilizando las ofertas de OpenAI y Cohere a través de las librerías `openai` y `cohere`.\n",
      "\n",
      "Pregunta: ¿Qué proveedores ofrecen modelos LLM?\n",
      "\n",
      "Respuesta:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mostrar el resultado del prompt generado\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d94ee-01bd-4086-889d-b337b81966f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 💡 Resultado esperado\n",
    "\n",
    "```\n",
    "Responde la pregunta con base en el contexto que aparece abajo.\n",
    "Si no se puede responder con la información proporcionada, responde con \"No lo sé\".\n",
    "\n",
    "Contexto: Los avances más recientes en PLN están siendo impulsados por Modelos de Lenguaje de Gran Escala (LLMs)...  \n",
    "Pregunta: ¿Qué proveedores ofrecen modelos LLM?\n",
    "\n",
    "Respuesta:\n",
    "```\n",
    "\n",
    "Este ejemplo toma un prompt estático y lo transforma en uno dinámico. La plantilla contiene la estructura del mensaje final junto con marcadores que indican dónde se insertarán las variables al momento de la ejecución.\n",
    "\n",
    "Así, la plantilla actúa como una receta reutilizable que puede generar múltiples prompts estáticos y específicos. Al formatear la plantilla con valores concretos —en este caso `context` y `question`— se obtiene un prompt listo para ser pasado a un modelo LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced4fd8-b5a5-4e5d-a0f2-e4b9ef3aef44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ⚙️ Ejecución del Prompt con un Modelo LLM (OpenAI)\n",
    "\n",
    "Como puedes ver, el argumento `question` se pasa de forma dinámica mediante la función `invoke()`. Por defecto, las plantillas de LangChain siguen la sintaxis de f-strings de Python para definir parámetros dinámicos—cualquier palabra entre llaves, como `{question}`, se sustituye con valores en tiempo de ejecución.  \n",
    "\n",
    "En el ejemplo anterior, `{question}` fue reemplazado por “¿Qué proveedores ofrecen modelos LLM?”\n",
    "\n",
    "Veamos ahora cómo alimentar este prompt en un modelo LLM de OpenAI usando LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4349f41-4cf6-40f1-8641-fe5f0a74a06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI y Cohere ofrecen modelos LLM.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 187, 'total_tokens': 203, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWTVnv5ZMv7pQSwpZuQ5MfmTlsT0', 'finish_reason': 'stop', 'logprobs': None} id='run-4eb4f07c-7b2e-42c8-a3c4-5c36eadd5e1e-0' usage_metadata={'input_tokens': 187, 'output_tokens': 16, 'total_tokens': 203, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Invocar el modelo con el prompt generado\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "# Mostrar respuesta\n",
    "print(completion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8fb59-7204-4df1-b232-572298985b83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ Resultado esperado\n",
    "\n",
    "```\n",
    "La librería `transformers` de Hugging Face, OpenAI a través de la librería `openai`, y Cohere mediante la librería `cohere` ofrecen modelos LLM.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Este enfoque permite separar claramente:\n",
    "- **La plantilla** (estructura reutilizable).\n",
    "- **Los datos** (como el contexto y la pregunta).\n",
    "- **El modelo** (el LLM que genera la respuesta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f3c5e-dd5a-42f2-82e2-ba81045269e2",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣️ Usando `ChatPromptTemplate` con Mensajes y Roles\n",
    "\n",
    "Observa cómo el prompt contiene instrucciones en un `SystemMessage` y dos instancias de `HumanMessage` que incluyen variables dinámicas como el contexto y la pregunta.  \n",
    "Aun así, puedes formatear la plantilla de la misma manera y obtener un prompt estático que puedes pasar a un modelo de lenguaje grande (LLM) para obtener una predicción.\n",
    "\n",
    "### 🐍 Código Python – `ChatPromptTemplate` con `ChatOpenAI`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688e0553-ea33-4bcc-9c70-5ee6e0f5d291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los proveedores que ofrecen Modelos de Lenguaje de Gran Escala (LLMs) incluyen Hugging Face, OpenAI y Cohere.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Plantilla de chat con mensajes diferenciados por rol\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"Responde la pregunta con base en el contexto a continuación. \n",
    "    Si no puedes responder con la información proporcionada, responde con \"No lo sé\".\"\"\"),\n",
    "    ('human', 'Contexto: {context}'),\n",
    "    ('human', 'Pregunta: {question}'),\n",
    "])\n",
    "\n",
    "# Instancia del modelo ChatOpenAI\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# Insertar valores dinámicos en el prompt\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"Los avances más recientes en PLN están siendo impulsados por \n",
    "    los Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus \n",
    "    contrapartes más pequeñas y se han vuelto invaluables para desarrolladores \n",
    "    que crean aplicaciones con capacidades de lenguaje natural. Los desarrolladores \n",
    "    pueden aprovechar estos modelos mediante la librería `transformers` de Hugging Face \n",
    "    o utilizando las ofertas de OpenAI y Cohere a través de las librerías `openai` y `cohere`.\"\"\",\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n",
    "# Invocar el modelo con el prompt generado\n",
    "respuesta = model.invoke(prompt)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed563f9d-0ef9-4fb9-a042-4044f6d74f15",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ✅ Resultado esperado\n",
    "\n",
    "```\n",
    "La librería `transformers` de Hugging Face, OpenAI mediante la librería `openai` y Cohere mediante la librería `cohere` ofrecen modelos LLM.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Este enfoque es ideal cuando estás desarrollando **chatbots o agentes conversacionales**, ya que te permite estructurar claramente los mensajes por rol (`system`, `human`, `assistant`) y reutilizar el mismo esquema para múltiples interacciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f45d41f-4b21-4cd0-a439-60b4338027bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Los proveedores que ofrecen Modelos de Lenguaje de Gran Escala (LLMs) incluyen Hugging Face, OpenAI y Cohere.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 190, 'total_tokens': 223, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWXmtXDDAZRIjUR2Qk2qwjYmt0wR', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a704890-940b-4f4c-8d5f-466efe482021-0', usage_metadata={'input_tokens': 190, 'output_tokens': 33, 'total_tokens': 223, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801822f-0ba7-4770-b047-aa7e90aaf58d",
   "metadata": {},
   "source": [
    "\n",
    "### 📦 Obtener Formatos Específicos desde LLMs\n",
    "\n",
    "Los resultados en texto plano son útiles, pero hay casos en los que necesitas que el modelo de lenguaje genere una salida estructurada, es decir, en un formato legible por máquina, como JSON, XML, CSV o incluso en un lenguaje de programación como Python o JavaScript. Esto es especialmente útil cuando esa salida será consumida por otra parte del sistema, haciendo que el LLM forme parte de una aplicación más amplia.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧾 Salida en Formato JSON\n",
    "\n",
    "El formato más común que se genera con LLMs es JSON. Este tipo de salida puede, por ejemplo, enviarse al frontend o almacenarse en una base de datos.\n",
    "\n",
    "Para generar JSON:\n",
    "1. Define el **esquema** que el modelo debe seguir.\n",
    "2. Incluye ese esquema en el prompt junto con el texto base.\n",
    "3. Usa el método `with_structured_output`.\n",
    "\n",
    "#### 🐍 Ejemplo en Python con `Pydantic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fdfb102-448a-439e-a99c-76c541f1e891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3552: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1660: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1673: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='Ambos pesan lo mismo, una libra.' justification='La libra es una unidad de medida de peso, por lo tanto, una libra de ladrillos y una libra de plumas tienen el mismo peso.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "# Definir el esquema de salida\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''Una respuesta a la pregunta del usuario junto con su justificación.'''\n",
    "    answer: str\n",
    "    '''La respuesta a la pregunta del usuario'''\n",
    "    justification: str\n",
    "    '''Justificación de la respuesta'''\n",
    "\n",
    "# Crear modelo base\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Adaptar el modelo para producir salida estructurada\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "# Invocar con una pregunta\n",
    "respuesta = structured_llm.invoke(\n",
    "    \"¿Qué pesa más, una libra de ladrillos o una libra de plumas?\"\n",
    ")\n",
    "\n",
    "# Mostrar resultado\n",
    "print(respuesta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6fb68-c9a0-46b4-b9e9-883ab5514cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### ✅ Resultado esperado\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"answer\": \"Pesan lo mismo\",\n",
    "  \"justification\": \"Tanto una libra de ladrillos como una libra de plumas pesan una libra. El peso es el mismo, aunque el volumen varía.\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 ¿Qué hace LangChain internamente?\n",
    "\n",
    "- Convierte el esquema Pydantic a **JSONSchema** y lo envía al modelo usando técnicas como *function calling* o *prompt injection*.\n",
    "- Valida que la respuesta generada respete el esquema antes de devolverla al usuario.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 Otros Formatos Legibles por Máquina con Output Parsers\n",
    "\n",
    "También puedes generar otros formatos como **CSV** o **XML**. Para eso, se utilizan **output parsers**, que son clases diseñadas para:\n",
    "\n",
    "#### 1. Inyectar instrucciones sobre el formato esperado en el prompt.  \n",
    "#### 2. Validar y transformar la respuesta del LLM en estructuras como listas o diccionarios.\n",
    "\n",
    "#### 🐍 Ejemplo en Python con `CommaSeparatedListOutputParser`\n",
    "\n",
    "LangChain ofrece múltiples parsers para distintos formatos: listas, JSON, XML, CSV y más.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74b59184-1d30-4335-895f-92d1553d8f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manzana', 'plátano', 'cereza']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Crear parser para lista separada por comas\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Aplicar el parser a una respuesta generada por LLM\n",
    "items = parser.invoke(\"manzana, plátano, cereza\")\n",
    "\n",
    "print(items)  # ['manzana', 'plátano', 'cereza']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f158cf-1d9e-481a-8bec-219ae33bad9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### 🧱 Ensamblando las Piezas de una Aplicación con LLM\n",
    "\n",
    "Los componentes clave que has aprendido hasta ahora son los bloques fundamentales del framework LangChain.  \n",
    "Lo que nos lleva a una pregunta crítica:\n",
    "\n",
    "> **¿Cómo combinarlos eficazmente para construir una aplicación con LLM?**\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Usando la Interfaz `Runnable`\n",
    "\n",
    "Como habrás notado, todos los ejemplos anteriores usan una interfaz común con el método `invoke()` para generar salidas desde el modelo, plantilla o parser.\n",
    "\n",
    "Todos los componentes implementan los siguientes métodos:\n",
    "\n",
    "| Método       | Función                                                                 |\n",
    "|--------------|-------------------------------------------------------------------------|\n",
    "| `invoke()`   | Transforma una **entrada individual** en una **salida**.                |\n",
    "| `batch()`    | Transforma **múltiples entradas** en **múltiples salidas**.             |\n",
    "| `stream()`   | Transmite la salida **en tiempo real** conforme se va generando.        |\n",
    "\n",
    "Además, hay utilidades integradas para:\n",
    "- Reintentos (`retries`)\n",
    "- Comportamientos de respaldo (`fallbacks`)\n",
    "- Validación de esquemas\n",
    "- Configuración dinámica en tiempo de ejecución\n",
    "\n",
    "> En Python, cada uno de estos métodos también tiene su versión asíncrona con `asyncio`.\n",
    "\n",
    "Esto permite que todos los componentes se comporten de manera **consistente**, y lo aprendido con uno se aplica a los demás.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfc4ac-9e91-4637-a453-26210afcb46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1decb9d7-b11d-45fb-a553-c04db3c45056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke: content='¡Hola! ¿Cómo puedo ayudarte hoy?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 10, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWjRWz6AQNg6Cylut3SR1N6wAztP', 'finish_reason': 'stop', 'logprobs': None} id='run-0e8c64a0-5921-4ee4-9c9a-8060fce85b5e-0' usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Crear modelo de chat\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# 1. invoke(): entrada única → salida única\n",
    "respuesta = model.invoke(\"¡Hola!\")\n",
    "print(\"invoke:\", respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4bbaf8-9e6d-4a22-9f06-3ae251c45cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: [AIMessage(content='¡Hola! ¿Cómo puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 10, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWjaB0PgUCICxGmg2cRYhMXyKjlM', 'finish_reason': 'stop', 'logprobs': None}, id='run-87039c00-d14c-4cc5-b160-d413d96c7d07-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='¡Adiós! Que tengas un buen día.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWjaZe4kZv9JxwdwczIRulX804WX', 'finish_reason': 'stop', 'logprobs': None}, id='run-187866a4-09b8-45e4-9b35-4f52e28bc2a3-0', usage_metadata={'input_tokens': 12, 'output_tokens': 13, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "# 2. batch(): múltiples entradas → múltiples salidas\n",
    "respuestas = model.batch([\"¡Hola!\", \"¡Adiós!\"])\n",
    "\n",
    "print(\"batch:\", respuestas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30637e9e-167f-4e23-a769-80f2257124d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream:\n",
      "content='' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='¡' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='Ad' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='i' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='ós' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='!' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' ¡' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='Que' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' teng' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='as' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' un' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' buen' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' día' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='!' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0613'} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' "
     ]
    }
   ],
   "source": [
    "# 3. stream(): salida generada en tiempo real\n",
    "print(\"stream:\")\n",
    "for token in model.stream(\"¡Hasta luego!\"):\n",
    "    print(token, end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672aa306-7de0-45e6-88dc-caaa7f514a81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 📊 ¿Cómo combinar componentes?\n",
    "\n",
    "LangChain te permite hacerlo de dos formas:\n",
    "\n",
    "| Enfoque         | Descripción                                                       |\n",
    "|------------------|-------------------------------------------------------------------|\n",
    "| **Imperativo**   | Llamas directamente a tus componentes (`model.invoke(...)`)       |\n",
    "| **Declarativo**  | Usas el **Lenguaje de Expresiones de LangChain (LCEL)**           |\n",
    "\n",
    "| Comparativa                  | Imperativo                          | Declarativo (LCEL)            |\n",
    "|-----------------------------|--------------------------------------|-------------------------------|\n",
    "| **Sintaxis**                | Todo Python o JavaScript             | LCEL                          |\n",
    "| **Ejecución en paralelo**   | Python: `threads` o `asyncio`        | Automática                    |\n",
    "|                             | JS: `Promise.all()`                  |                               |\n",
    "| **Streaming**               | Con `yield`                          | Automático                    |\n",
    "| **Ejecución asíncrona**     | Con `async`                          | Automática                    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127c50d-8650-4b0f-a77d-84e3d97494f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### ⚙️ Composición Imperativa en LangChain\n",
    "\n",
    "La **composición imperativa** no es más que el enfoque tradicional de escribir funciones y clases, combinando componentes como modelos, prompts y parsers de manera explícita. Es familiar, flexible y te permite agregar cualquier lógica personalizada que necesites.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Ejemplo básico: Combinando prompt + modelo en una función\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b3f786-91cd-4c14-98b1-3bbb45a4811a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM (Master en Derecho) es un programa de posgrado en derecho. Sin embargo, si te refieres a modelos de lenguaje de aprendizaje automático (LLM), hay varios proveedores que ofrecen estos servicios, incluyendo:\n",
      "\n",
      "1. OpenAI: Ofrecen GPT-3, uno de los modelos de lenguaje más avanzados disponibles actualmente.\n",
      "\n",
      "2. Google: Ofrece BERT y T5, que son modelos de lenguaje transformacional.\n",
      "\n",
      "3. Facebook AI: Ofrece RoBERTa, que es una variante de BERT.\n",
      "\n",
      "4. Microsoft: Ofrece Turing, que es un modelo de lenguaje de gran escala.\n",
      "\n",
      "5. Hugging Face: Ofrece una amplia gama de modelos de lenguaje pre-entrenados, incluyendo BERT, GPT-2, DistilBERT, RoBERTa, y muchos más.\n",
      "\n",
      "6. IBM: Ofrece Watson, que es un sistema de inteligencia artificial que puede procesar el lenguaje natural.\n",
      "\n",
      "Por favor, especifica si necesitas más detalles sobre un proveedor en particular o si te refieres a algo diferente con \"modelos LLM\".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# 1. Definimos el prompt del sistema + mensaje humano\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Eres un asistente útil.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "# 2. Modelo de lenguaje\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# 3. Función combinada usando @chain\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# 4. Usar el chatbot\n",
    "respuesta = chatbot.invoke({\"question\": \"¿Qué proveedores ofrecen modelos LLM?\"})\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8b826-cabb-41f8-b23d-b4c7e1df86fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 🔁 Soporte para *streaming* (respuesta en partes)\n",
    "\n",
    "Puedes modificar la función para que haga *streaming* de la respuesta del modelo en tiempo real:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a12ffb7-85fd-4b59-8b85-430913878bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LL M  ( Language  Model )  es  un  tipo  de  modelo  de  intelig encia  artificial  que  se  utiliza  para  la  gener ación  de  texto  y  el  proces amiento  del  l engu aje  natural .  Aqu í  hay  algunos  prove edores  que  ofrec en  modelos  L LM :\n",
      "\n",
      " 1 .  Open AI :  Of rece  G PT - 3 ,  uno  de  los  modelos  de  l engu aje  más  avanz ados  disponibles  en  la  actual idad .\n",
      "\n",
      " 2 .  Google :  Of rece  B ERT  y  T 5 ,  que  son  modelos  de  l engu aje  que  se  utiliz an  para  una  varied ad  de  t areas  de  proces amiento  del  l engu aje  natural .\n",
      "\n",
      " 3 .  Facebook  AI :  Of rece  Ro BERT a ,  que  es  una  vari ante  de  B ERT  que  ha  sido  optim izada  para  tener  un  rend imiento  aún  mejor .\n",
      "\n",
      " 4 .  H ugging  Face :  Of rece  una  varied ad  de  modelos  de  l engu aje  pre - ent ren ados ,  incl uy endo  B ERT ,  G PT - 2 ,  y  Ro BERT a .\n",
      "\n",
      " 5 .  Microsoft :  Of rece  Turing ,  un  modelo  de  l engu aje  de  gran  esc ala  que  se  utiliza  en  Bing  y  otras  aplic aciones  de  Microsoft .\n",
      "\n",
      " 6 .  IBM :  Of rece  Watson ,  que  utiliza  modelos  de  l engu aje  para  una  varied ad  de  t areas ,  incl uy endo  la  comp rens ión  y  gener ación  de  l engu aje  natural .\n",
      "\n",
      " 7 .  Amazon :  Of rece  Com preh end ,  un  servicio  que  utiliza  modelos  de  l engu aje  para  anal izar  texto .  "
     ]
    }
   ],
   "source": [
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "# Usar streaming\n",
    "for parte in chatbot.stream({\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "}):\n",
    "    print(parte.content, end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39761458-f1ff-4de0-8ba2-b5cbf3888a3d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Esto te devuelve la respuesta en fragmentos (`AIMessageChunk`), útil para interfaces conversacionales en tiempo real.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ Soporte para ejecución *asíncrona* (async/await)\n",
    "\n",
    "En Python, también puedes definir tu función de manera asíncrona:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2035ff-3774-458e-8635-e484d6709938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1134/1193923704.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m respuesta = asyncio.run(chatbot.ainvoke({\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"¿Qué proveedores ofrecen modelos LLM?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m }))\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "# Usar async en un entorno compatible con asyncio\n",
    "import asyncio\n",
    "\n",
    "respuesta = asyncio.run(chatbot.ainvoke({\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "}))\n",
    "\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec32bcd-906c-4963-9d85-4547515b59ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "El mensaje `RuntimeError: asyncio.run() cannot be called from a running event loop` indica que ya hay un bucle de eventos de asyncio en ejecución en el entorno donde estás intentando correr este código. Esto es común en entornos como los notebooks de Jupyter, donde el kernel ya tiene un bucle de eventos en marcha para manejar las operaciones asíncronas.\n",
    "\n",
    "Para solucionar esto en un entorno donde ya existe un bucle de eventos, en lugar de usar `asyncio.run()`, puedes usar `asyncio.create_task()` para programar la corrutina y luego `await` la tarea resultante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f533be-529d-4599-81d2-ac5ff147819c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algunos proveedores que ofrecen modelos LLM son BERT, GPT-3 de OpenAI, RoBERTa de Facebook AI Research, T5 de Google Research, entre otros proveedores de tecnología de procesamiento del lenguaje natural.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "\n",
    "# Definir el template del prompt\n",
    "template = PromptTemplate.from_template(\"Responde a la siguiente pregunta: {question}\")\n",
    "\n",
    "# Inicializar el modelo de chat de OpenAI (asegúrate de tener tu API key configurada)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Definir la cadena (chain) asíncrona\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "# Crear una tarea para ejecutar la corrutina chatbot\n",
    "task = asyncio.create_task(chatbot({\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "}))\n",
    "\n",
    "# Esperar a que la tarea se complete y obtener el resultado\n",
    "respuesta = await task\n",
    "\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2812b-9e11-4df9-9166-77c0cddd528f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ✅ Resultado Esperado\n",
    "\n",
    "```\n",
    "Hugging Face con la librería `transformers`, OpenAI mediante `openai` y Cohere con `cohere` ofrecen modelos LLM.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Ventajas de este enfoque imperativo\n",
    "\n",
    "- Es familiar para cualquier desarrollador Python.\n",
    "- Puedes insertar **cualquier lógica personalizada** (if/else, validaciones, llamadas externas).\n",
    "- Compatible con streaming y async cuando lo necesitas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7048500-4f14-4ba6-97d9-6b050b36dd15",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧩 Composición Declarativa con LCEL\n",
    "\n",
    "**LCEL (LangChain Expression Language)** es un lenguaje declarativo para **componer componentes de LangChain**. LangChain compila estas composiciones en **planes de ejecución optimizados**, ofreciendo:\n",
    "\n",
    "- Paralelización automática\n",
    "- Soporte para *streaming*\n",
    "- Ejecución asíncrona\n",
    "- *Tracing* automático (seguimiento del flujo)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧱 Ejemplo equivalente al chatbot, ahora con LCEL\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Componentes base\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Eres un asistente útil.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# Composición declarativa con el operador \"|\"\n",
    "chatbot = template | model\n",
    "\n",
    "# Invocar el chatbot\n",
    "respuesta = chatbot.invoke({\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n",
    "print(respuesta.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Streaming automático (sin lógica adicional)\n",
    "\n",
    "```python\n",
    "chatbot = template | model\n",
    "\n",
    "for parte in chatbot.stream({\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "}):\n",
    "    print(parte.content, end=\" \")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ Asincronía automática\n",
    "\n",
    "```python\n",
    "# Esto dentro de una celda async-compatible\n",
    "respuesta = await chatbot.ainvoke({\n",
    "    \"question\": \"¿Qué proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n",
    "print(respuesta.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Resumen del capítulo\n",
    "\n",
    "En esta sección aprendiste a usar los **componentes clave** de LangChain para construir aplicaciones con LLMs:\n",
    "\n",
    "- **Modelos de lenguaje**: Hacen las predicciones.\n",
    "- **Prompts**: Guían la respuesta del modelo.\n",
    "- **Parsers (opcional)**: Estructuran la salida del modelo.\n",
    "\n",
    "Todos los componentes comparten una interfaz común:\n",
    "\n",
    "| Método       | Función                                            |\n",
    "|--------------|----------------------------------------------------|\n",
    "| `invoke()`   | Entrada única → Salida única                        |\n",
    "| `batch()`    | Entradas múltiples → Salidas múltiples              |\n",
    "| `stream()`   | Generación progresiva de respuesta (token a token) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🆚 Composición imperativa vs. declarativa\n",
    "\n",
    "| Característica           | Imperativa                            | Declarativa (LCEL)               |\n",
    "|--------------------------|----------------------------------------|----------------------------------|\n",
    "| Sintaxis                 | Python clásico                         | Expresiones encadenadas con `| `  |\n",
    "| Lógica personalizada     | 🟢 Sí, altamente personalizable         | 🔴 Limitada (solo composición)   |\n",
    "| Paralelismo / Async      | Manual (asyncio, threads)              | Automático                       |\n",
    "| Streaming                | Requiere `yield` o `stream()` manual   | Automático                       |\n",
    "\n",
    "---\n",
    "\n",
    "> En el siguiente capítulo aprenderás cómo **proveer datos externos a tu chatbot** como contexto, para que puedas crear una aplicación que realmente pueda \"conversar\" con tus datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477ba61-7947-4671-ae74-dd45b9faf85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
