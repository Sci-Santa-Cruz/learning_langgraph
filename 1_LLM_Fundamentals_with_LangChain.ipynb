{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7347c1-3047-4d83-a0d9-993f05c3726b",
   "metadata": {},
   "source": [
    "# Learning LangChain\n",
    "by Mayo Oshin, Nuno Campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1078e4e4-0adc-4a8b-a534-9ebb1877e11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0faa1354-89ae-43c1-bbf4-23716de587b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_LLM_Fundamentals_with_LangChain.ipynb  Arquitecturas.ipynb  memory.ipynb\n",
      "Agentes_I.ipynb                          Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6bda04-3734-4a5c-8e81-3873d0d8e56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ir al directorio principal\n",
    "from os import chdir\n",
    "\n",
    "# chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b77fb5-d530-4b19-b586-55ae93b1fed0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'learning_langgraph': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "ls learning_langgraph -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58da26b0-5657-4abd-9df8-c6536936c6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cargar varibles\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58015001-255f-4607-8c60-d11f89fe9e5a",
   "metadata": {},
   "source": [
    "\n",
    "# **Cap√≠tulo 1. Fundamentos de LLM con LangChain**\n",
    "\n",
    "El prefacio te dio una probadita del poder del *prompting* de LLM, donde vimos de primera mano el impacto que diferentes t√©cnicas de *prompting* pueden tener en lo que obtienes de los LLM, especialmente cuando se combinan con criterio. El desaf√≠o al construir buenas aplicaciones de LLM radica, de hecho, en c√≥mo construir eficazmente el *prompt* enviado al modelo y procesar la predicci√≥n del modelo para devolver una salida precisa (ver Figura 1-1).\n",
    "\n",
    "**Figura 1-1. El desaf√≠o de hacer que los LLM sean una parte √∫til de tu aplicaci√≥n**\n",
    "\n",
    "Si puedes resolver este problema, est√°s en buen camino para construir aplicaciones de LLM, tanto simples como complejas. En este cap√≠tulo, aprender√°s m√°s sobre c√≥mo los bloques de construcci√≥n de LangChain se mapean a los conceptos de LLM y c√≥mo, cuando se combinan eficazmente, te permiten construir aplicaciones de LLM. Pero primero, la barra lateral \"¬øPor qu√© LangChain?\" es una breve introducci√≥n de por qu√© creemos que es √∫til usar LangChain para construir aplicaciones de LLM.\n",
    "\n",
    "**¬øPOR QU√â LANGCHAIN?**\n",
    "\n",
    "Por supuesto, puedes construir aplicaciones de LLM sin LangChain. La alternativa m√°s obvia es usar el kit de desarrollo de software (SDK) ‚Äîel paquete que expone los m√©todos de su API HTTP como funciones en el lenguaje de programaci√≥n de tu elecci√≥n‚Äî del proveedor de LLM que probaste primero (por ejemplo, OpenAI). Creemos que aprender LangChain valdr√° la pena a corto y largo plazo debido a los siguientes factores:\n",
    "\n",
    "**Patrones comunes preconstruidos**\n",
    "\n",
    "LangChain viene con implementaciones de referencia de los patrones de aplicaci√≥n de LLM m√°s comunes (mencionamos algunos de estos en el prefacio: cadena de pensamiento, llamada a herramientas y otros). Esta es la forma m√°s r√°pida de comenzar con los LLM y, a menudo, podr√≠a ser todo lo que necesitas. Sugerimos comenzar cualquier aplicaci√≥n nueva a partir de estos y verificar si los resultados listos para usar son lo suficientemente buenos para tu caso de uso. Si no, consulta el siguiente punto para la otra mitad de las bibliotecas de LangChain.\n",
    "\n",
    "**Bloques de construcci√≥n intercambiables**\n",
    "\n",
    "Estos son componentes que se pueden intercambiar f√°cilmente por alternativas. Cada componente (un LLM, un modelo de chat, un analizador de salida, etc.‚Äîm√°s sobre esto en breve) sigue una especificaci√≥n compartida, lo que hace que tu aplicaci√≥n est√© preparada para el futuro. A medida que los proveedores de modelos lanzan nuevas capacidades y a medida que cambian tus necesidades, puedes hacer evolucionar tu aplicaci√≥n sin tener que reescribirla cada vez.\n",
    "\n",
    "A lo largo de este libro, utilizamos los siguientes componentes principales en los ejemplos de c√≥digo:\n",
    "\n",
    "* **LLM/modelo de chat:** OpenAI\n",
    "* **Embeddings:** OpenAI\n",
    "* **Almac√©n de vectores:** PGVector\n",
    "\n",
    "Puedes intercambiar cada uno de estos por cualquiera de las alternativas que se enumeran en las siguientes p√°ginas:\n",
    "\n",
    "**Modelos de chat**\n",
    "\n",
    "Consulta la documentaci√≥n de LangChain. Si no quieres usar OpenAI (una API comercial), te sugerimos Anthropic como una alternativa comercial u Ollama como una de c√≥digo abierto.\n",
    "\n",
    "**Embeddings**\n",
    "\n",
    "Consulta la documentaci√≥n de LangChain. Si no quieres usar OpenAI (una API comercial), te sugerimos Cohere como una alternativa comercial u Ollama como una de c√≥digo abierto.\n",
    "\n",
    "**Almacenes de vectores**\n",
    "\n",
    "Consulta la documentaci√≥n de LangChain. Si no quieres usar PGVector (una extensi√≥n de c√≥digo abierto para la popular base de datos SQL Postgres), te sugerimos usar Weaviate (un almac√©n de vectores dedicado) u OpenSearch (caracter√≠sticas de b√∫squeda vectorial que forman parte de una popular base de datos de b√∫squeda).\n",
    "\n",
    "Este esfuerzo va m√°s all√°, por ejemplo, de que todos los LLM tengan los mismos m√©todos, con argumentos y valores de retorno similares. Veamos el ejemplo de los modelos de chat y dos proveedores populares de LLM, OpenAI y Anthropic. Ambos tienen una API de chat que recibe mensajes de chat (definidos vagamente como objetos con un string de tipo y un string de contenido) y devuelve un nuevo mensaje generado por el modelo. Pero si intentas usar ambos modelos en la misma conversaci√≥n, inmediatamente encontrar√°s problemas, ya que sus formatos de mensajes de chat son sutilmente incompatibles. LangChain abstrae estas diferencias para permitir la construcci√≥n de aplicaciones que son verdaderamente independientes de un proveedor en particular. Por ejemplo, con LangChain, una conversaci√≥n de chatbot en la que utilizas modelos de OpenAI y Anthropic funciona.\n",
    "\n",
    "Finalmente, a medida que desarrollas tus aplicaciones de LLM con varios de estos componentes, nos ha resultado √∫til contar con las capacidades de orquestaci√≥n de LangChain:\n",
    "\n",
    "* Todos los componentes principales est√°n instrumentados por el sistema de *callbacks* para la observabilidad (m√°s sobre esto en el Cap√≠tulo 8).\n",
    "* Todos los componentes principales implementan la misma interfaz (m√°s sobre esto hacia el final de este cap√≠tulo).\n",
    "* Las aplicaciones de LLM de larga duraci√≥n se pueden interrumpir, reanudar o reintentar (m√°s sobre esto en el Cap√≠tulo 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7c7f9-1828-4687-8384-196b0ef3004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lelc_0101 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0fe98-9a75-4c9b-8c69-1da053622deb",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# **Configuraci√≥n de LangChain**\n",
    "\n",
    "Para seguir el resto del cap√≠tulo y los cap√≠tulos venideros, te recomendamos configurar LangChain en tu computadora primero.\n",
    "\n",
    "Consulta las instrucciones en el Prefacio con respecto a la creaci√≥n de una cuenta de OpenAI y completa estos pasos si a√∫n no lo has hecho. Si prefieres usar un proveedor de LLM diferente, consulta \"¬øPor qu√© LangChain?\" para ver alternativas.\n",
    "\n",
    "Luego, dir√≠gete a la p√°gina de Claves API en el sitio web de OpenAI (despu√©s de iniciar sesi√≥n en tu cuenta de OpenAI), crea una clave API y gu√°rdala; la necesitar√°s pronto.\n",
    "\n",
    "**NOTA**\n",
    "\n",
    "En este libro, mostraremos ejemplos de c√≥digo tanto en Python como en JavaScript (JS). LangChain ofrece la misma funcionalidad en ambos lenguajes, as√≠ que simplemente elige el que te resulte m√°s c√≥modo y sigue los fragmentos de c√≥digo correspondientes a lo largo del libro (los ejemplos de c√≥digo para cada lenguaje son equivalentes).\n",
    "\n",
    "Primero, algunas instrucciones de configuraci√≥n para los lectores que usan Python:\n",
    "\n",
    "Aseg√∫rate de tener Python instalado. Consulta las instrucciones para tu sistema operativo.\n",
    "\n",
    "Instala Jupyter si deseas ejecutar los ejemplos en un entorno de *notebook*. Puedes hacerlo ejecutando `pip install notebook` en tu terminal.\n",
    "\n",
    "Instala la biblioteca LangChain ejecutando los siguientes comandos en tu terminal:\n",
    "\n",
    "```\n",
    "pip install langchain langchain-openai langchain-community\n",
    "pip install langchain-text-splitters langchain-postgres\n",
    "```\n",
    "\n",
    "Toma la clave API de OpenAI que generaste al principio de esta secci√≥n y hazla disponible en tu entorno de terminal. Puedes hacerlo ejecutando lo siguiente:\n",
    "\n",
    "```\n",
    "export OPENAI_API_KEY=tu-clave\n",
    "```\n",
    "\n",
    "No olvides reemplazar `tu-clave` con la clave API que generaste anteriormente.\n",
    "\n",
    "Abre un *notebook* de Jupyter ejecutando este comando:\n",
    "\n",
    "```\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Ahora est√°s listo para seguir los ejemplos de c√≥digo de Python.\n",
    "\n",
    "**Uso de LLM en LangChain**\n",
    "\n",
    "Para recapitular, los LLM son el motor impulsor detr√°s de la mayor√≠a de las aplicaciones de IA generativa. LangChain proporciona dos interfaces simples para interactuar con cualquier proveedor de API de LLM:\n",
    "\n",
    "Modelos de chat\n",
    "\n",
    "LLM\n",
    "\n",
    "La interfaz LLM simplemente toma un *prompt* de cadena como entrada, env√≠a la entrada al proveedor del modelo y luego devuelve la predicci√≥n del modelo como salida.\n",
    "\n",
    "Importemos el *wrapper* OpenAI LLM de LangChain para invocar una predicci√≥n del modelo utilizando un *prompt* simple:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c423053-5a33-4f33-9243-8cd8ed75a9f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clear and sunny today.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "model.invoke(\"The sky is\").content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301431a5-a4e4-42fa-8006-6a402a5b92bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "**CONSEJO**\n",
    "\n",
    "Observa el par√°metro `model` pasado a `OpenAI`. Este es el par√°metro m√°s com√∫n para configurar al usar un LLM o un modelo de chat, el modelo subyacente a utilizar, ya que la mayor√≠a de los proveedores ofrecen varios modelos con diferentes compensaciones en capacidad y costo (generalmente los modelos m√°s grandes son m√°s capaces, pero tambi√©n m√°s caros y lentos). Consulta la descripci√≥n general de los modelos que ofrece OpenAI.\n",
    "\n",
    "Otros par√°metros √∫tiles para configurar incluyen los siguientes, ofrecidos por la mayor√≠a de los proveedores:\n",
    "\n",
    "* `temperature`: Controla el algoritmo de muestreo utilizado para generar la salida. Los valores m√°s bajos producen salidas m√°s predecibles (por ejemplo, 0.1), mientras que los valores m√°s altos generan resultados m√°s creativos o inesperados (como 0.9). Diferentes tareas necesitar√°n diferentes valores para este par√°metro. Por ejemplo, la producci√≥n de salida estructurada generalmente se beneficia de una temperatura m√°s baja, mientras que las tareas de escritura creativa funcionan mejor con un valor m√°s alto.\n",
    "* `max_tokens`: Limita el tama√±o (y el costo) de la salida. Un valor m√°s bajo puede hacer que el LLM deje de generar la salida antes de llegar a un final natural, por lo que puede parecer que se ha truncado.\n",
    "\n",
    "M√°s all√° de estos, cada proveedor expone un conjunto diferente de par√°metros. Te recomendamos consultar la documentaci√≥n del que elijas. Por ejemplo, consulta la plataforma de OpenAI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336184ae-3234-4ba4-8d78-ce02507bc250",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Alternativamente, la interfaz de modelo de chat permite conversaciones de ida y vuelta entre el usuario y el modelo. La raz√≥n por la que es una interfaz separada es porque los proveedores populares de modelos de lenguaje como OpenAI diferencian los mensajes enviados hacia y desde el modelo en roles de *usuario*, *asistente* y *sistema* (aqu√≠, el *rol* denota el tipo de contenido que contiene el mensaje):\n",
    "\n",
    "**Rol del sistema**  \n",
    "Se utiliza para dar instrucciones que el modelo debe seguir para responder a una pregunta del usuario.\n",
    "\n",
    "**Rol del usuario**  \n",
    "Se utiliza para la consulta del usuario y cualquier otro contenido producido por este.\n",
    "\n",
    "**Rol del asistente**  \n",
    "Se utiliza para el contenido generado por el modelo.\n",
    "\n",
    "La interfaz del modelo de chat facilita la configuraci√≥n y gesti√≥n de las conversaciones en tu aplicaci√≥n de chatbot con IA.  \n",
    "A continuaci√≥n, se muestra un ejemplo utilizando el modelo `ChatOpenAI` de LangChain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7066eb80-84ed-453e-a9b5-ea7f3bc3bda7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNWFWydRDDAKPNqCmHABrb5Pya6jf', 'finish_reason': 'stop', 'logprobs': None}, id='run-c1e1caa2-a2c3-430e-9077-1e53bc9361b9-0', usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Configuraci√≥n del modelo con par√°metros personalizados\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.7,              # Controla la creatividad del modelo (0 = determinista, 1 = aleatorio)\n",
    "    model_name=\"gpt-3.5\",           # Puedes cambiarlo por \"gpt-3.5-turbo\" u otros modelos compatibles\n",
    "    max_tokens=500,               # L√≠mite de tokens que puede generar el modelo en una respuesta\n",
    "    openai_api_key=\"tu_clave_aqu√≠\"  # (opcional si ya lo configuraste en las variables de entorno)\n",
    ")\n",
    "\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab5290-ae42-4c9e-9537-5964e27c4288",
   "metadata": {},
   "source": [
    "### Otros par√°metros √∫tiles que puedes incluir:\n",
    "\n",
    "- `top_p`: alternativa a `temperature`, controla la \"nucleaci√≥n\" de las respuestas.\n",
    "- `frequency_penalty`: penaliza la repetici√≥n de palabras o frases.\n",
    "- `presence_penalty`: incentiva hablar de nuevos temas.\n",
    "- `streaming`: si quieres respuestas en tiempo real (requiere configuraci√≥n adicional).\n",
    "- `request_timeout`: para controlar cu√°nto tiempo esperas antes de que la petici√≥n falle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aea12b-896c-44a5-a84b-584c0bf47f97",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "En lugar de utilizar un √∫nico string como prompt, los modelos de chat hacen uso de distintos tipos de mensajes, asociados a cada uno de los roles mencionados anteriormente. Estos incluyen lo siguiente:\n",
    "\n",
    "- **HumanMessage**  \n",
    "  Un mensaje enviado desde la perspectiva del humano, con el rol de *usuario*.\n",
    "\n",
    "- **AIMessage**  \n",
    "  Un mensaje enviado desde la perspectiva de la IA, con el rol de *asistente*.\n",
    "\n",
    "- **SystemMessage**  \n",
    "  Un mensaje que define las instrucciones que la IA debe seguir, con el rol de *sistema*.\n",
    "\n",
    "- **ChatMessage**  \n",
    "  Un mensaje que permite establecer un rol personalizado de manera arbitraria.\n",
    "\n",
    "Vamos a incorporar una instrucci√≥n con `SystemMessage` en nuestro ejemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19053981-c829-4ff7-9baa-551e25890a8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de Francia es Par√≠s!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "# Crear instancia del modelo\n",
    "model = ChatOpenAI(\n",
    "    temperature=0.5,        # Puedes ajustar este valor para controlar la aleatoriedad\n",
    "    model_name=\"gpt-4\",     # Especifica el modelo a utilizar\n",
    "    max_tokens=100          # L√≠mite de tokens en la respuesta\n",
    ")\n",
    "\n",
    "# Mensaje del sistema con instrucciones\n",
    "system_msg = SystemMessage(\n",
    "    '''Eres un asistente √∫til que responde a las preguntas con tres signos \n",
    "    de exclamaci√≥n.'''\n",
    ")\n",
    "\n",
    "# Mensaje del usuario\n",
    "human_msg = HumanMessage(\"¬øCu√°l es la capital de Francia?\")\n",
    "\n",
    "# Ejecutar el modelo con los mensajes\n",
    "respuesta = model.invoke([system_msg, human_msg])\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(respuesta.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c91ab-7887-4964-94ea-a612135f6f7c",
   "metadata": {},
   "source": [
    "### üß† Salida esperada:\n",
    "\n",
    "```\n",
    "¬°Par√≠s!!!\n",
    "```\n",
    "\n",
    "Como puedes ver, el modelo obedeci√≥ la instrucci√≥n proporcionada en el `SystemMessage`, a pesar de que esta instrucci√≥n no estaba presente en la pregunta del usuario. Esto te permite preconfigurar el comportamiento de tu aplicaci√≥n de IA para que responda de manera relativamente predecible seg√∫n la entrada del usuario.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e341ec-6942-456d-9c83-c7d0d2cf79aa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üîÅ Haciendo Reutilizables los Prompts en Modelos LLM\n",
    "\n",
    "La secci√≥n anterior mostr√≥ c√≥mo las instrucciones del prompt influyen significativamente en la salida del modelo. Los prompts ayudan al modelo a entender el contexto y generar respuestas relevantes a las consultas.\n",
    "\n",
    "A continuaci√≥n, se muestra un ejemplo de un prompt detallado:\n",
    "\n",
    "```\n",
    "Responde la pregunta con base en el contexto que aparece abajo.  \n",
    "Si no se puede responder con la informaci√≥n proporcionada, responde con \"No lo s√©\".\n",
    "\n",
    "Contexto: Los avances m√°s recientes en PLN (Procesamiento de Lenguaje Natural) est√°n siendo impulsados por los Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus contrapartes m√°s peque√±as y se han vuelto invaluables para los desarrolladores que crean aplicaciones con capacidades de PLN. Los desarrolladores pueden aprovechar estos modelos mediante la librer√≠a `transformers` de Hugging Face, o utilizando las ofertas de OpenAI y Cohere a trav√©s de las librer√≠as `openai` y `cohere`, respectivamente.\n",
    "\n",
    "Pregunta: ¬øQu√© proveedores ofrecen modelos LLM?\n",
    "\n",
    "Respuesta:\n",
    "```\n",
    "\n",
    "Aunque el prompt parece ser solo un string simple, el verdadero reto es decidir qu√© debe contener ese texto y c√≥mo debe adaptarse din√°micamente a las entradas del usuario.\n",
    "\n",
    "Por suerte, **LangChain** proporciona interfaces de plantillas de prompt que hacen f√°cil construir instrucciones con entradas din√°micas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "330bc9d9-b57b-45f0-8cbc-4b8deeec4e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Crear la plantilla con marcadores din√°micos\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta con base en el contexto que aparece abajo.\n",
    "Si no se puede responder con la informaci√≥n proporcionada, responde con \"No lo s√©\".\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805986af-d7bf-4e19-bd25-6fc1f53eb20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Invocar la plantilla con valores din√°micos\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"Los avances m√°s recientes en PLN est√°n siendo impulsados por \n",
    "    Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus \n",
    "    contrapartes m√°s peque√±as y se han vuelto invaluables para los desarrolladores \n",
    "    que crean aplicaciones con capacidades de lenguaje natural. Los desarrolladores \n",
    "    pueden aprovechar estos modelos mediante la librer√≠a `transformers` de Hugging Face \n",
    "    o utilizando las ofertas de OpenAI y Cohere a trav√©s de las librer√≠as `openai` y `cohere`.\"\"\",\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88f4cc5a-7c48-4b8f-9ace-f8775bdb5533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responde la pregunta con base en el contexto que aparece abajo.\n",
      "Si no se puede responder con la informaci√≥n proporcionada, responde con \"No lo s√©\".\n",
      "\n",
      "Contexto: Los avances m√°s recientes en PLN est√°n siendo impulsados por \n",
      "    Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus \n",
      "    contrapartes m√°s peque√±as y se han vuelto invaluables para los desarrolladores \n",
      "    que crean aplicaciones con capacidades de lenguaje natural. Los desarrolladores \n",
      "    pueden aprovechar estos modelos mediante la librer√≠a `transformers` de Hugging Face \n",
      "    o utilizando las ofertas de OpenAI y Cohere a trav√©s de las librer√≠as `openai` y `cohere`.\n",
      "\n",
      "Pregunta: ¬øQu√© proveedores ofrecen modelos LLM?\n",
      "\n",
      "Respuesta:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mostrar el resultado del prompt generado\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d94ee-01bd-4086-889d-b337b81966f4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üí° Resultado esperado\n",
    "\n",
    "```\n",
    "Responde la pregunta con base en el contexto que aparece abajo.\n",
    "Si no se puede responder con la informaci√≥n proporcionada, responde con \"No lo s√©\".\n",
    "\n",
    "Contexto: Los avances m√°s recientes en PLN est√°n siendo impulsados por Modelos de Lenguaje de Gran Escala (LLMs)...  \n",
    "Pregunta: ¬øQu√© proveedores ofrecen modelos LLM?\n",
    "\n",
    "Respuesta:\n",
    "```\n",
    "\n",
    "Este ejemplo toma un prompt est√°tico y lo transforma en uno din√°mico. La plantilla contiene la estructura del mensaje final junto con marcadores que indican d√≥nde se insertar√°n las variables al momento de la ejecuci√≥n.\n",
    "\n",
    "As√≠, la plantilla act√∫a como una receta reutilizable que puede generar m√∫ltiples prompts est√°ticos y espec√≠ficos. Al formatear la plantilla con valores concretos ‚Äîen este caso `context` y `question`‚Äî se obtiene un prompt listo para ser pasado a un modelo LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced4fd8-b5a5-4e5d-a0f2-e4b9ef3aef44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚öôÔ∏è Ejecuci√≥n del Prompt con un Modelo LLM (OpenAI)\n",
    "\n",
    "Como puedes ver, el argumento `question` se pasa de forma din√°mica mediante la funci√≥n `invoke()`. Por defecto, las plantillas de LangChain siguen la sintaxis de f-strings de Python para definir par√°metros din√°micos‚Äîcualquier palabra entre llaves, como `{question}`, se sustituye con valores en tiempo de ejecuci√≥n.  \n",
    "\n",
    "En el ejemplo anterior, `{question}` fue reemplazado por ‚Äú¬øQu√© proveedores ofrecen modelos LLM?‚Äù\n",
    "\n",
    "Veamos ahora c√≥mo alimentar este prompt en un modelo LLM de OpenAI usando LangChain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4349f41-4cf6-40f1-8641-fe5f0a74a06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI y Cohere ofrecen modelos LLM.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 187, 'total_tokens': 203, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWTVnv5ZMv7pQSwpZuQ5MfmTlsT0', 'finish_reason': 'stop', 'logprobs': None} id='run-4eb4f07c-7b2e-42c8-a3c4-5c36eadd5e1e-0' usage_metadata={'input_tokens': 187, 'output_tokens': 16, 'total_tokens': 203, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Invocar el modelo con el prompt generado\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "# Mostrar respuesta\n",
    "print(completion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8fb59-7204-4df1-b232-572298985b83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚úÖ Resultado esperado\n",
    "\n",
    "```\n",
    "La librer√≠a `transformers` de Hugging Face, OpenAI a trav√©s de la librer√≠a `openai`, y Cohere mediante la librer√≠a `cohere` ofrecen modelos LLM.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Este enfoque permite separar claramente:\n",
    "- **La plantilla** (estructura reutilizable).\n",
    "- **Los datos** (como el contexto y la pregunta).\n",
    "- **El modelo** (el LLM que genera la respuesta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f3c5e-dd5a-42f2-82e2-ba81045269e2",
   "metadata": {},
   "source": [
    "\n",
    "### üó£Ô∏è Usando `ChatPromptTemplate` con Mensajes y Roles\n",
    "\n",
    "Observa c√≥mo el prompt contiene instrucciones en un `SystemMessage` y dos instancias de `HumanMessage` que incluyen variables din√°micas como el contexto y la pregunta.  \n",
    "Aun as√≠, puedes formatear la plantilla de la misma manera y obtener un prompt est√°tico que puedes pasar a un modelo de lenguaje grande (LLM) para obtener una predicci√≥n.\n",
    "\n",
    "### üêç C√≥digo Python ‚Äì `ChatPromptTemplate` con `ChatOpenAI`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688e0553-ea33-4bcc-9c70-5ee6e0f5d291",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los proveedores que ofrecen Modelos de Lenguaje de Gran Escala (LLMs) incluyen Hugging Face, OpenAI y Cohere.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Plantilla de chat con mensajes diferenciados por rol\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"\"\"Responde la pregunta con base en el contexto a continuaci√≥n. \n",
    "    Si no puedes responder con la informaci√≥n proporcionada, responde con \"No lo s√©\".\"\"\"),\n",
    "    ('human', 'Contexto: {context}'),\n",
    "    ('human', 'Pregunta: {question}'),\n",
    "])\n",
    "\n",
    "# Instancia del modelo ChatOpenAI\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# Insertar valores din√°micos en el prompt\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"\"\"Los avances m√°s recientes en PLN est√°n siendo impulsados por \n",
    "    los Modelos de Lenguaje de Gran Escala (LLMs). Estos modelos superan a sus \n",
    "    contrapartes m√°s peque√±as y se han vuelto invaluables para desarrolladores \n",
    "    que crean aplicaciones con capacidades de lenguaje natural. Los desarrolladores \n",
    "    pueden aprovechar estos modelos mediante la librer√≠a `transformers` de Hugging Face \n",
    "    o utilizando las ofertas de OpenAI y Cohere a trav√©s de las librer√≠as `openai` y `cohere`.\"\"\",\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n",
    "# Invocar el modelo con el prompt generado\n",
    "respuesta = model.invoke(prompt)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed563f9d-0ef9-4fb9-a042-4044f6d74f15",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚úÖ Resultado esperado\n",
    "\n",
    "```\n",
    "La librer√≠a `transformers` de Hugging Face, OpenAI mediante la librer√≠a `openai` y Cohere mediante la librer√≠a `cohere` ofrecen modelos LLM.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Este enfoque es ideal cuando est√°s desarrollando **chatbots o agentes conversacionales**, ya que te permite estructurar claramente los mensajes por rol (`system`, `human`, `assistant`) y reutilizar el mismo esquema para m√∫ltiples interacciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f45d41f-4b21-4cd0-a439-60b4338027bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Los proveedores que ofrecen Modelos de Lenguaje de Gran Escala (LLMs) incluyen Hugging Face, OpenAI y Cohere.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 190, 'total_tokens': 223, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWXmtXDDAZRIjUR2Qk2qwjYmt0wR', 'finish_reason': 'stop', 'logprobs': None}, id='run-9a704890-940b-4f4c-8d5f-466efe482021-0', usage_metadata={'input_tokens': 190, 'output_tokens': 33, 'total_tokens': 223, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801822f-0ba7-4770-b047-aa7e90aaf58d",
   "metadata": {},
   "source": [
    "\n",
    "### üì¶ Obtener Formatos Espec√≠ficos desde LLMs\n",
    "\n",
    "Los resultados en texto plano son √∫tiles, pero hay casos en los que necesitas que el modelo de lenguaje genere una salida estructurada, es decir, en un formato legible por m√°quina, como JSON, XML, CSV o incluso en un lenguaje de programaci√≥n como Python o JavaScript. Esto es especialmente √∫til cuando esa salida ser√° consumida por otra parte del sistema, haciendo que el LLM forme parte de una aplicaci√≥n m√°s amplia.\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Salida en Formato JSON\n",
    "\n",
    "El formato m√°s com√∫n que se genera con LLMs es JSON. Este tipo de salida puede, por ejemplo, enviarse al frontend o almacenarse en una base de datos.\n",
    "\n",
    "Para generar JSON:\n",
    "1. Define el **esquema** que el modelo debe seguir.\n",
    "2. Incluye ese esquema en el prompt junto con el texto base.\n",
    "3. Usa el m√©todo `with_structured_output`.\n",
    "\n",
    "#### üêç Ejemplo en Python con `Pydantic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fdfb102-448a-439e-a99c-76c541f1e891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3552: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1660: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:1673: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='Ambos pesan lo mismo, una libra.' justification='La libra es una unidad de medida de peso, por lo tanto, una libra de ladrillos y una libra de plumas tienen el mismo peso.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "# Definir el esquema de salida\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    '''Una respuesta a la pregunta del usuario junto con su justificaci√≥n.'''\n",
    "    answer: str\n",
    "    '''La respuesta a la pregunta del usuario'''\n",
    "    justification: str\n",
    "    '''Justificaci√≥n de la respuesta'''\n",
    "\n",
    "# Crear modelo base\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Adaptar el modelo para producir salida estructurada\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "\n",
    "# Invocar con una pregunta\n",
    "respuesta = structured_llm.invoke(\n",
    "    \"¬øQu√© pesa m√°s, una libra de ladrillos o una libra de plumas?\"\n",
    ")\n",
    "\n",
    "# Mostrar resultado\n",
    "print(respuesta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6fb68-c9a0-46b4-b9e9-883ab5514cab",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### ‚úÖ Resultado esperado\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"answer\": \"Pesan lo mismo\",\n",
    "  \"justification\": \"Tanto una libra de ladrillos como una libra de plumas pesan una libra. El peso es el mismo, aunque el volumen var√≠a.\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîç ¬øQu√© hace LangChain internamente?\n",
    "\n",
    "- Convierte el esquema Pydantic a **JSONSchema** y lo env√≠a al modelo usando t√©cnicas como *function calling* o *prompt injection*.\n",
    "- Valida que la respuesta generada respete el esquema antes de devolverla al usuario.\n",
    "\n",
    "---\n",
    "\n",
    "### üß∞ Otros Formatos Legibles por M√°quina con Output Parsers\n",
    "\n",
    "Tambi√©n puedes generar otros formatos como **CSV** o **XML**. Para eso, se utilizan **output parsers**, que son clases dise√±adas para:\n",
    "\n",
    "#### 1. Inyectar instrucciones sobre el formato esperado en el prompt.  \n",
    "#### 2. Validar y transformar la respuesta del LLM en estructuras como listas o diccionarios.\n",
    "\n",
    "#### üêç Ejemplo en Python con `CommaSeparatedListOutputParser`\n",
    "\n",
    "LangChain ofrece m√∫ltiples parsers para distintos formatos: listas, JSON, XML, CSV y m√°s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74b59184-1d30-4335-895f-92d1553d8f21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manzana', 'pl√°tano', 'cereza']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Crear parser para lista separada por comas\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Aplicar el parser a una respuesta generada por LLM\n",
    "items = parser.invoke(\"manzana, pl√°tano, cereza\")\n",
    "\n",
    "print(items)  # ['manzana', 'pl√°tano', 'cereza']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f158cf-1d9e-481a-8bec-219ae33bad9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### üß± Ensamblando las Piezas de una Aplicaci√≥n con LLM\n",
    "\n",
    "Los componentes clave que has aprendido hasta ahora son los bloques fundamentales del framework LangChain.  \n",
    "Lo que nos lleva a una pregunta cr√≠tica:\n",
    "\n",
    "> **¬øC√≥mo combinarlos eficazmente para construir una aplicaci√≥n con LLM?**\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Usando la Interfaz `Runnable`\n",
    "\n",
    "Como habr√°s notado, todos los ejemplos anteriores usan una interfaz com√∫n con el m√©todo `invoke()` para generar salidas desde el modelo, plantilla o parser.\n",
    "\n",
    "Todos los componentes implementan los siguientes m√©todos:\n",
    "\n",
    "| M√©todo       | Funci√≥n                                                                 |\n",
    "|--------------|-------------------------------------------------------------------------|\n",
    "| `invoke()`   | Transforma una **entrada individual** en una **salida**.                |\n",
    "| `batch()`    | Transforma **m√∫ltiples entradas** en **m√∫ltiples salidas**.             |\n",
    "| `stream()`   | Transmite la salida **en tiempo real** conforme se va generando.        |\n",
    "\n",
    "Adem√°s, hay utilidades integradas para:\n",
    "- Reintentos (`retries`)\n",
    "- Comportamientos de respaldo (`fallbacks`)\n",
    "- Validaci√≥n de esquemas\n",
    "- Configuraci√≥n din√°mica en tiempo de ejecuci√≥n\n",
    "\n",
    "> En Python, cada uno de estos m√©todos tambi√©n tiene su versi√≥n as√≠ncrona con `asyncio`.\n",
    "\n",
    "Esto permite que todos los componentes se comporten de manera **consistente**, y lo aprendido con uno se aplica a los dem√°s.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfc4ac-9e91-4637-a453-26210afcb46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1decb9d7-b11d-45fb-a553-c04db3c45056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke: content='¬°Hola! ¬øC√≥mo puedo ayudarte hoy?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 10, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWjRWz6AQNg6Cylut3SR1N6wAztP', 'finish_reason': 'stop', 'logprobs': None} id='run-0e8c64a0-5921-4ee4-9c9a-8060fce85b5e-0' usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Crear modelo de chat\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# 1. invoke(): entrada √∫nica ‚Üí salida √∫nica\n",
    "respuesta = model.invoke(\"¬°Hola!\")\n",
    "print(\"invoke:\", respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c4bbaf8-9e6d-4a22-9f06-3ae251c45cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: [AIMessage(content='¬°Hola! ¬øC√≥mo puedo ayudarte hoy?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 10, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWjaB0PgUCICxGmg2cRYhMXyKjlM', 'finish_reason': 'stop', 'logprobs': None}, id='run-87039c00-d14c-4cc5-b160-d413d96c7d07-0', usage_metadata={'input_tokens': 10, 'output_tokens': 11, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='¬°Adi√≥s! Que tengas un buen d√≠a.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 12, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BNWjaZe4kZv9JxwdwczIRulX804WX', 'finish_reason': 'stop', 'logprobs': None}, id='run-187866a4-09b8-45e4-9b35-4f52e28bc2a3-0', usage_metadata={'input_tokens': 12, 'output_tokens': 13, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "# 2. batch(): m√∫ltiples entradas ‚Üí m√∫ltiples salidas\n",
    "respuestas = model.batch([\"¬°Hola!\", \"¬°Adi√≥s!\"])\n",
    "\n",
    "print(\"batch:\", respuestas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30637e9e-167f-4e23-a769-80f2257124d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream:\n",
      "content='' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='¬°' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='Ad' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='i' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='√≥s' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='!' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' ¬°' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='Que' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' teng' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='as' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' un' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' buen' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content=' d√≠a' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='!' additional_kwargs={} response_metadata={} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4-0613'} id='run-9b48d5c7-458a-4a10-a3eb-f21c9bf0eaf5' "
     ]
    }
   ],
   "source": [
    "# 3. stream(): salida generada en tiempo real\n",
    "print(\"stream:\")\n",
    "for token in model.stream(\"¬°Hasta luego!\"):\n",
    "    print(token, end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672aa306-7de0-45e6-88dc-caaa7f514a81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìä ¬øC√≥mo combinar componentes?\n",
    "\n",
    "LangChain te permite hacerlo de dos formas:\n",
    "\n",
    "| Enfoque         | Descripci√≥n                                                       |\n",
    "|------------------|-------------------------------------------------------------------|\n",
    "| **Imperativo**   | Llamas directamente a tus componentes (`model.invoke(...)`)       |\n",
    "| **Declarativo**  | Usas el **Lenguaje de Expresiones de LangChain (LCEL)**           |\n",
    "\n",
    "| Comparativa                  | Imperativo                          | Declarativo (LCEL)            |\n",
    "|-----------------------------|--------------------------------------|-------------------------------|\n",
    "| **Sintaxis**                | Todo Python o JavaScript             | LCEL                          |\n",
    "| **Ejecuci√≥n en paralelo**   | Python: `threads` o `asyncio`        | Autom√°tica                    |\n",
    "|                             | JS: `Promise.all()`                  |                               |\n",
    "| **Streaming**               | Con `yield`                          | Autom√°tico                    |\n",
    "| **Ejecuci√≥n as√≠ncrona**     | Con `async`                          | Autom√°tica                    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b127c50d-8650-4b0f-a77d-84e3d97494f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### ‚öôÔ∏è Composici√≥n Imperativa en LangChain\n",
    "\n",
    "La **composici√≥n imperativa** no es m√°s que el enfoque tradicional de escribir funciones y clases, combinando componentes como modelos, prompts y parsers de manera expl√≠cita. Es familiar, flexible y te permite agregar cualquier l√≥gica personalizada que necesites.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Ejemplo b√°sico: Combinando prompt + modelo en una funci√≥n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b3f786-91cd-4c14-98b1-3bbb45a4811a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM (Master en Derecho) es un programa de posgrado en derecho. Sin embargo, si te refieres a modelos de lenguaje de aprendizaje autom√°tico (LLM), hay varios proveedores que ofrecen estos servicios, incluyendo:\n",
      "\n",
      "1. OpenAI: Ofrecen GPT-3, uno de los modelos de lenguaje m√°s avanzados disponibles actualmente.\n",
      "\n",
      "2. Google: Ofrece BERT y T5, que son modelos de lenguaje transformacional.\n",
      "\n",
      "3. Facebook AI: Ofrece RoBERTa, que es una variante de BERT.\n",
      "\n",
      "4. Microsoft: Ofrece Turing, que es un modelo de lenguaje de gran escala.\n",
      "\n",
      "5. Hugging Face: Ofrece una amplia gama de modelos de lenguaje pre-entrenados, incluyendo BERT, GPT-2, DistilBERT, RoBERTa, y muchos m√°s.\n",
      "\n",
      "6. IBM: Ofrece Watson, que es un sistema de inteligencia artificial que puede procesar el lenguaje natural.\n",
      "\n",
      "Por favor, especifica si necesitas m√°s detalles sobre un proveedor en particular o si te refieres a algo diferente con \"modelos LLM\".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "# 1. Definimos el prompt del sistema + mensaje humano\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Eres un asistente √∫til.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "# 2. Modelo de lenguaje\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# 3. Funci√≥n combinada usando @chain\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "# 4. Usar el chatbot\n",
    "respuesta = chatbot.invoke({\"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"})\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8b826-cabb-41f8-b23d-b4c7e1df86fa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üîÅ Soporte para *streaming* (respuesta en partes)\n",
    "\n",
    "Puedes modificar la funci√≥n para que haga *streaming* de la respuesta del modelo en tiempo real:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a12ffb7-85fd-4b59-8b85-430913878bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LL M  ( Language  Model )  es  un  tipo  de  modelo  de  intelig encia  artificial  que  se  utiliza  para  la  gener aci√≥n  de  texto  y  el  proces amiento  del  l engu aje  natural .  Aqu √≠  hay  algunos  prove edores  que  ofrec en  modelos  L LM :\n",
      "\n",
      " 1 .  Open AI :  Of rece  G PT - 3 ,  uno  de  los  modelos  de  l engu aje  m√°s  avanz ados  disponibles  en  la  actual idad .\n",
      "\n",
      " 2 .  Google :  Of rece  B ERT  y  T 5 ,  que  son  modelos  de  l engu aje  que  se  utiliz an  para  una  varied ad  de  t areas  de  proces amiento  del  l engu aje  natural .\n",
      "\n",
      " 3 .  Facebook  AI :  Of rece  Ro BERT a ,  que  es  una  vari ante  de  B ERT  que  ha  sido  optim izada  para  tener  un  rend imiento  a√∫n  mejor .\n",
      "\n",
      " 4 .  H ugging  Face :  Of rece  una  varied ad  de  modelos  de  l engu aje  pre - ent ren ados ,  incl uy endo  B ERT ,  G PT - 2 ,  y  Ro BERT a .\n",
      "\n",
      " 5 .  Microsoft :  Of rece  Turing ,  un  modelo  de  l engu aje  de  gran  esc ala  que  se  utiliza  en  Bing  y  otras  aplic aciones  de  Microsoft .\n",
      "\n",
      " 6 .  IBM :  Of rece  Watson ,  que  utiliza  modelos  de  l engu aje  para  una  varied ad  de  t areas ,  incl uy endo  la  comp rens i√≥n  y  gener aci√≥n  de  l engu aje  natural .\n",
      "\n",
      " 7 .  Amazon :  Of rece  Com preh end ,  un  servicio  que  utiliza  modelos  de  l engu aje  para  anal izar  texto .  "
     ]
    }
   ],
   "source": [
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "# Usar streaming\n",
    "for parte in chatbot.stream({\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "}):\n",
    "    print(parte.content, end=\" \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39761458-f1ff-4de0-8ba2-b5cbf3888a3d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Esto te devuelve la respuesta en fragmentos (`AIMessageChunk`), √∫til para interfaces conversacionales en tiempo real.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Soporte para ejecuci√≥n *as√≠ncrona* (async/await)\n",
    "\n",
    "En Python, tambi√©n puedes definir tu funci√≥n de manera as√≠ncrona:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2035ff-3774-458e-8635-e484d6709938",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1134/1193923704.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m respuesta = asyncio.run(chatbot.ainvoke({\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"¬øQu√© proveedores ofrecen modelos LLM?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m }))\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "# Usar async en un entorno compatible con asyncio\n",
    "import asyncio\n",
    "\n",
    "respuesta = asyncio.run(chatbot.ainvoke({\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "}))\n",
    "\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec32bcd-906c-4963-9d85-4547515b59ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "El mensaje `RuntimeError: asyncio.run() cannot be called from a running event loop` indica que ya hay un bucle de eventos de asyncio en ejecuci√≥n en el entorno donde est√°s intentando correr este c√≥digo. Esto es com√∫n en entornos como los notebooks de Jupyter, donde el kernel ya tiene un bucle de eventos en marcha para manejar las operaciones as√≠ncronas.\n",
    "\n",
    "Para solucionar esto en un entorno donde ya existe un bucle de eventos, en lugar de usar `asyncio.run()`, puedes usar `asyncio.create_task()` para programar la corrutina y luego `await` la tarea resultante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f533be-529d-4599-81d2-ac5ff147819c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algunos proveedores que ofrecen modelos LLM son BERT, GPT-3 de OpenAI, RoBERTa de Facebook AI Research, T5 de Google Research, entre otros proveedores de tecnolog√≠a de procesamiento del lenguaje natural.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import asyncio\n",
    "\n",
    "# Definir el template del prompt\n",
    "template = PromptTemplate.from_template(\"Responde a la siguiente pregunta: {question}\")\n",
    "\n",
    "# Inicializar el modelo de chat de OpenAI (aseg√∫rate de tener tu API key configurada)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Definir la cadena (chain) as√≠ncrona\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "# Crear una tarea para ejecutar la corrutina chatbot\n",
    "task = asyncio.create_task(chatbot({\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "}))\n",
    "\n",
    "# Esperar a que la tarea se complete y obtener el resultado\n",
    "respuesta = await task\n",
    "\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2812b-9e11-4df9-9166-77c0cddd528f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Resultado Esperado\n",
    "\n",
    "```\n",
    "Hugging Face con la librer√≠a `transformers`, OpenAI mediante `openai` y Cohere con `cohere` ofrecen modelos LLM.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Ventajas de este enfoque imperativo\n",
    "\n",
    "- Es familiar para cualquier desarrollador Python.\n",
    "- Puedes insertar **cualquier l√≥gica personalizada** (if/else, validaciones, llamadas externas).\n",
    "- Compatible con streaming y async cuando lo necesitas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7048500-4f14-4ba6-97d9-6b050b36dd15",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß© Composici√≥n Declarativa con LCEL\n",
    "\n",
    "**LCEL (LangChain Expression Language)** es un lenguaje declarativo para **componer componentes de LangChain**. LangChain compila estas composiciones en **planes de ejecuci√≥n optimizados**, ofreciendo:\n",
    "\n",
    "- Paralelizaci√≥n autom√°tica\n",
    "- Soporte para *streaming*\n",
    "- Ejecuci√≥n as√≠ncrona\n",
    "- *Tracing* autom√°tico (seguimiento del flujo)\n",
    "\n",
    "---\n",
    "\n",
    "### üß± Ejemplo equivalente al chatbot, ahora con LCEL\n",
    "\n",
    "```python\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Componentes base\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Eres un asistente √∫til.'),\n",
    "    ('human', '{question}'),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# Composici√≥n declarativa con el operador \"|\"\n",
    "chatbot = template | model\n",
    "\n",
    "# Invocar el chatbot\n",
    "respuesta = chatbot.invoke({\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n",
    "print(respuesta.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Streaming autom√°tico (sin l√≥gica adicional)\n",
    "\n",
    "```python\n",
    "chatbot = template | model\n",
    "\n",
    "for parte in chatbot.stream({\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "}):\n",
    "    print(parte.content, end=\" \")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Asincron√≠a autom√°tica\n",
    "\n",
    "```python\n",
    "# Esto dentro de una celda async-compatible\n",
    "respuesta = await chatbot.ainvoke({\n",
    "    \"question\": \"¬øQu√© proveedores ofrecen modelos LLM?\"\n",
    "})\n",
    "\n",
    "print(respuesta.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Resumen del cap√≠tulo\n",
    "\n",
    "En esta secci√≥n aprendiste a usar los **componentes clave** de LangChain para construir aplicaciones con LLMs:\n",
    "\n",
    "- **Modelos de lenguaje**: Hacen las predicciones.\n",
    "- **Prompts**: Gu√≠an la respuesta del modelo.\n",
    "- **Parsers (opcional)**: Estructuran la salida del modelo.\n",
    "\n",
    "Todos los componentes comparten una interfaz com√∫n:\n",
    "\n",
    "| M√©todo       | Funci√≥n                                            |\n",
    "|--------------|----------------------------------------------------|\n",
    "| `invoke()`   | Entrada √∫nica ‚Üí Salida √∫nica                        |\n",
    "| `batch()`    | Entradas m√∫ltiples ‚Üí Salidas m√∫ltiples              |\n",
    "| `stream()`   | Generaci√≥n progresiva de respuesta (token a token) |\n",
    "\n",
    "---\n",
    "\n",
    "### üÜö Composici√≥n imperativa vs. declarativa\n",
    "\n",
    "| Caracter√≠stica           | Imperativa                            | Declarativa (LCEL)               |\n",
    "|--------------------------|----------------------------------------|----------------------------------|\n",
    "| Sintaxis                 | Python cl√°sico                         | Expresiones encadenadas con `| `  |\n",
    "| L√≥gica personalizada     | üü¢ S√≠, altamente personalizable         | üî¥ Limitada (solo composici√≥n)   |\n",
    "| Paralelismo / Async      | Manual (asyncio, threads)              | Autom√°tico                       |\n",
    "| Streaming                | Requiere `yield` o `stream()` manual   | Autom√°tico                       |\n",
    "\n",
    "---\n",
    "\n",
    "> En el siguiente cap√≠tulo aprender√°s c√≥mo **proveer datos externos a tu chatbot** como contexto, para que puedas crear una aplicaci√≥n que realmente pueda \"conversar\" con tus datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477ba61-7947-4671-ae74-dd45b9faf85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
