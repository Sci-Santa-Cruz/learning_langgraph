{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192e28a-725f-4b6a-a058-fc1e1a54165c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615fcfdd-0e51-4f38-8929-af874bdc2521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = key  # reemplaza con tu clave real si es necesario\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c364ee-eb4d-4af8-b535-c1fdc0b5a63b",
   "metadata": {},
   "source": [
    "\n",
    "## Usando LangGraph para Agregar Memoria a tu Chatbot\n",
    "\n",
    "En el Cap√≠tulo 3, aprendiste c√≥mo proporcionar a tu aplicaci√≥n de chatbot con IA un contexto actualizado y relevante. Esto le permite generar respuestas precisas basadas en lo que el usuario escribe. Pero eso no es suficiente para tener una aplicaci√≥n lista para producci√≥n. ¬øC√≥mo puedes hacer que tu chatbot realmente converse de ida y vuelta con el usuario, recordando lo que se ha dicho antes?\n",
    "\n",
    "Los modelos de lenguaje grandes (**LLMs**) son **sin estado**, lo que significa que cada vez que se les da un nuevo mensaje, no recuerdan el mensaje anterior ni sus propias respuestas. Para que puedan ‚Äúrecordar‚Äù conversaciones pasadas, necesitamos un sistema de memoria que lleve el seguimiento de los intercambios anteriores y del contexto relevante. Esta informaci√≥n hist√≥rica puede incluirse en el prompt final que se env√≠a al modelo, d√°ndole as√≠ una ‚Äúmemoria‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "## Construyendo un Sistema de Memoria para un Chatbot\n",
    "\n",
    "Hay dos decisiones clave que tomar al dise√±ar un sistema de memoria:\n",
    "\n",
    "1. **C√≥mo se almacena el estado**\n",
    "2. **C√≥mo se consulta ese estado**\n",
    "\n",
    "Una forma simple y efectiva de construir un sistema de memoria es guardar todo el historial de la conversaci√≥n entre el usuario y el modelo, y reutilizarlo. El estado de este sistema puede:\n",
    "\n",
    "- Guardarse como una lista de mensajes (ver Cap√≠tulo 1 para m√°s detalles sobre el formato)\n",
    "- Actualizarse agregando los mensajes m√°s recientes despu√©s de cada turno\n",
    "- Insertarse en el prompt para que el modelo lo tenga en cuenta\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo de implementaci√≥n simple con LangChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f237dfaf-eaee-46ec-9950-2b7971969503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acabo de decir en franc√©s \"J'adore programmer\", que significa \"Me encanta programar\" en espa√±ol.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Definimos el prompt que incluye un mensaje del sistema y un marcador para los mensajes anteriores\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente √∫til. Responde todas las preguntas lo mejor que puedas.\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Instanciamos el modelo de chat\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Encadenamos el prompt y el modelo\n",
    "chain = prompt | model\n",
    "\n",
    "# Simulamos una conversaci√≥n previa\n",
    "respuesta = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Traduce esta oraci√≥n del ingl√©s al franc√©s: I love programming.\"),\n",
    "        (\"ai\", \"J'adore programmer.\"),\n",
    "        (\"human\", \"¬øQu√© acabas de decir?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Mostramos la respuesta generada por el modelo\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491b3e5-bb71-4a21-a779-beb308863496",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Resultado esperado:\n",
    "\n",
    "```\n",
    "Dije: \"J'adore programmer\", que significa \"I love programming\" en franc√©s.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Limitaciones y desaf√≠os en producci√≥n\n",
    "\n",
    "Este enfoque simple funciona, pero en producci√≥n enfrentar√°s algunos retos adicionales:\n",
    "\n",
    "- Es necesario actualizar la memoria **despu√©s de cada interacci√≥n**, de forma **at√≥mica** (registrar tanto la pregunta como la respuesta).\n",
    "- Se recomienda almacenar la memoria en un sistema duradero, como una base de datos relacional.\n",
    "- Se debe controlar **cu√°ntos mensajes** se almacenan y cu√°ntos se usan en cada nueva interacci√≥n.\n",
    "- Es √∫til poder **inspeccionar y modificar el estado** (la lista de mensajes) desde fuera del flujo de ejecuci√≥n del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d733c4-4fa3-4dfd-8e78-0721bbe4dd3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introducci√≥n a LangGraph\n",
    "\n",
    "A partir de este cap√≠tulo y durante los siguientes, comenzaremos a utilizar **LangGraph**, una biblioteca de c√≥digo abierto creada por **LangChain**. LangGraph est√° dise√±ada para permitir a los desarrolladores implementar **arquitecturas cognitivas con m√∫ltiples actores, m√∫ltiples pasos y con estado**, conocidas como *graphs* o **grafos**.\n",
    "\n",
    "Eso puede sonar como muchos conceptos en una sola oraci√≥n, as√≠ que vamos a desglosarlo.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones con m√∫ltiples actores\n",
    "\n",
    "![Figura 4-3: De aplicaciones con un solo actor a aplicaciones con m√∫ltiples actores]\n",
    "\n",
    "Al igual que un equipo de especialistas puede lograr cosas que una sola persona no podr√≠a, las aplicaciones basadas en LLMs tambi√©n pueden beneficiarse cuando se combinan con otras herramientas. Por ejemplo:\n",
    "\n",
    "- Un **prompt de LLM** es excelente para generaci√≥n de texto o planeaci√≥n de tareas.\n",
    "- Un **buscador web** es ideal para encontrar hechos actuales.\n",
    "\n",
    "Cuando combinas ambos ‚Äîe incluso varios LLMs diferentes entre s√≠‚Äî puedes crear aplicaciones innovadoras como **Perplexity** o **Arc Search**, que combinan estos elementos para lograr algo mucho m√°s potente.\n",
    "\n",
    "As√≠ como los equipos humanos necesitan **coordinaci√≥n**, una aplicaci√≥n con m√∫ltiples actores tambi√©n necesita una **capa de coordinaci√≥n** para:\n",
    "\n",
    "- Definir los actores involucrados (los **nodos** del grafo) y c√≥mo se comunican entre s√≠ (las **aristas**).\n",
    "- Programar la ejecuci√≥n de cada actor en el momento adecuado, incluso en **paralelo** y de forma **determinista**.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones con m√∫ltiples pasos\n",
    "\n",
    "![Figura 4-4: De m√∫ltiples actores a m√∫ltiples pasos]\n",
    "\n",
    "Cuando un actor entrega trabajo a otro (por ejemplo, un LLM que hace una pregunta a una herramienta de b√∫squeda), el sistema necesita saber:\n",
    "\n",
    "- En qu√© orden ocurren los eventos.\n",
    "- Cu√°ntas veces se llama a cada actor.\n",
    "- Cu√°ndo se detiene el proceso.\n",
    "\n",
    "Esto se puede modelar como una **secuencia de pasos discretos**. Cada paso representa una transferencia de trabajo, hasta que ya no quedan tareas pendientes y se llega a un resultado final.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones con estado\n",
    "\n",
    "![Figura 4-5: De m√∫ltiples pasos a aplicaciones con estado]\n",
    "\n",
    "Para coordinar acciones entre pasos, se necesita **mantener un estado**. Si no se hace esto, el LLM dar√≠a la misma respuesta cada vez que se le llama. Al centralizar el estado:\n",
    "\n",
    "- Se puede **almacenar un snapshot** del estado en cualquier momento.\n",
    "- Se puede **pausar y reanudar** la ejecuci√≥n, √∫til para recuperaci√≥n de errores.\n",
    "- Se pueden **integrar humanos en el flujo de trabajo** (m√°s sobre esto en el Cap√≠tulo 8).\n",
    "\n",
    "---\n",
    "\n",
    "## Componentes de un grafo en LangGraph\n",
    "\n",
    "Cada grafo en LangGraph tiene tres elementos clave:\n",
    "\n",
    "1. **Estado (State)**  \n",
    "   Datos que entran a la aplicaci√≥n y se modifican mientras el grafo est√° en ejecuci√≥n.\n",
    "\n",
    "2. **Nodos (Nodes)**  \n",
    "   Pasos individuales representados como funciones Python. Cada funci√≥n recibe el estado actual, lo modifica (si es necesario) y devuelve una nueva versi√≥n del estado.\n",
    "\n",
    "3. **Aristas (Edges)**  \n",
    "   Conexiones entre los nodos. Estas pueden ser:\n",
    "   - **Fijas**: por ejemplo, despu√©s del nodo B siempre va el nodo D.\n",
    "   - **Condicionales**: se eval√∫a una funci√≥n para decidir a qu√© nodo ir despu√©s.\n",
    "\n",
    "LangGraph tambi√©n permite visualizar estos grafos y ofrece herramientas √∫tiles para depuraci√≥n y despliegue en producci√≥n a gran escala.\n",
    "\n",
    "---\n",
    "\n",
    "## Instalaci√≥n de LangGraph\n",
    "\n",
    "Si seguiste las instrucciones del Cap√≠tulo 1, ya deber√≠as tener LangGraph instalado. Si no, puedes instalarlo con:\n",
    "\n",
    "```bash\n",
    "pip install langgraph\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Primer ejemplo: Un chatbot simple con LangGraph\n",
    "\n",
    "Para familiarizarnos con LangGraph, construiremos un chatbot simple. Este responder√° directamente a los mensajes del usuario utilizando un solo LLM. Aunque es un ejemplo sencillo, ilustra los **principios clave** de LangGraph:\n",
    "\n",
    "- C√≥mo se define un nodo.\n",
    "- C√≥mo se actualiza el estado.\n",
    "- C√≥mo fluye la informaci√≥n en el grafo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364555a-4d81-4f4e-9649-e27363dae5a1",
   "metadata": {},
   "source": [
    "Claro, Omar. Aqu√≠ tienes el contenido traducido, reorganizado y con **todo el c√≥digo Python en una sola celda ejecutable** al final, como lo pediste:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Creaci√≥n de un `StateGraph` con LangGraph (Python)\n",
    "\n",
    "### üìã Descripci√≥n paso a paso\n",
    "\n",
    "Para comenzar a usar **LangGraph**, lo primero que debes hacer es definir el **estado del grafo**. Este estado define:\n",
    "\n",
    "1. La estructura del estado compartido entre nodos (por ejemplo, un historial de mensajes).\n",
    "2. C√≥mo se actualiza cada parte del estado cuando un nodo devuelve un nuevo valor.\n",
    "3. En este ejemplo, usamos `add_messages`, que indica que los mensajes nuevos deben **agregarse** a la lista existente, en lugar de sobrescribirla.\n",
    "\n",
    "### üí¨ Estado y Nodo\n",
    "\n",
    "Definimos una clase `State` que representa el estado compartido y un nodo llamado `chatbot`, que usa un modelo de lenguaje (`ChatOpenAI`) para generar una respuesta basada en los mensajes anteriores. El resultado se agrega nuevamente al estado gracias a la anotaci√≥n `add_messages`.\n",
    "\n",
    "### üß© Conexi√≥n de nodos y compilaci√≥n\n",
    "\n",
    "Luego, conectamos los nodos al punto de inicio (`START`) y final (`END`), y compilamos el grafo en un objeto ejecutable.\n",
    "\n",
    "### üìä Visualizaci√≥n y ejecuci√≥n\n",
    "\n",
    "Puedes visualizar el grafo generado con `draw_mermaid_png()` y ejecutarlo con `stream`, que ir√° devolviendo el estado tras cada paso del grafo.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ C√≥digo Python completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014298e1-7752-4003-af8c-384ef08aaf76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øQu√© proveedores ofrecen modelos LLM?\n",
      "LLM (Language Model) es un tipo de modelo de aprendizaje autom√°tico que se utiliza en el procesamiento del lenguaje natural. Algunos proveedores que ofrecen modelos LLM incluyen:\n",
      "\n",
      "1. OpenAI: Ofrece GPT-3, uno de los modelos de lenguaje m√°s avanzados disponibles. \n",
      "\n",
      "2. Google: Ofrece BERT y T5, que son modelos de lenguaje que pueden ser utilizados para una variedad de tareas de procesamiento de lenguaje natural.\n",
      "\n",
      "3. Facebook AI: Ofrece RoBERTa, que es una variante de BERT que ha sido optimizada para tener un rendimiento a√∫n mejor.\n",
      "\n",
      "4. Microsoft: Ofrece Turing, un modelo de lenguaje que ha sido entrenado en una gran cantidad de texto de internet.\n",
      "\n",
      "5. Hugging Face: Ofrece una variedad de modelos de lenguaje pre-entrenados que pueden ser utilizados para una variedad de tareas de procesamiento de lenguaje natural.\n",
      "\n",
      "6. IBM: Ofrece Watson, que incluye capacidades de procesamiento de lenguaje natural y puede ser utilizado para una variedad de tareas.\n",
      "\n",
      "7. Amazon: Ofrece Comprehend, un servicio que utiliza el aprendizaje autom√°tico para descubrir informaci√≥n en texto.\n",
      "\n",
      "8. Alibaba Cloud: Ofrece un modelo de lenguaje basado en la nube para el procesamiento del lenguaje natural. \n",
      "\n",
      "Estos son solo algunos ejemplos y hay muchos otros proveedores que ofrecen modelos de lenguaje.\n"
     ]
    }
   ],
   "source": [
    "# LangGraph b√°sico con un solo nodo de ChatOpenAI (versi√≥n compatible)\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "# 1. Definir el esquema del estado (forma segura con TypedDict)\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "# 2. Instanciar modelo LLM\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# 3. Definir el nodo chatbot\n",
    "def chatbot(state: GraphState):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "# 4. Construir el grafo\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. Ejecutar el grafo con mensaje inicial\n",
    "entrada = {\"messages\": [HumanMessage(content=\"¬øQu√© proveedores ofrecen modelos LLM?\")]}\n",
    "respuesta = graph.invoke(entrada)\n",
    "\n",
    "# 6. Mostrar respuesta del modelo\n",
    "for mensaje in respuesta[\"messages\"]:\n",
    "    print(mensaje.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bdff3-001f-4c99-98a5-6341ff6f45c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agregar memoria a `StateGraph`\n",
    "\n",
    "LangGraph tiene persistencia integrada, que se utiliza de la misma forma tanto para los grafos m√°s simples como para los m√°s complejos. Veamos c√≥mo se aplica esto a la arquitectura inicial.\n",
    "\n",
    "Vamos a recompilar nuestro grafo, pero ahora adjuntando un *checkpointer*, que es un adaptador de almacenamiento para LangGraph. LangGraph incluye una clase base que cualquier usuario puede extender para crear un adaptador para su base de datos favorita. Hasta el momento de esta escritura, LangGraph incluye varios adaptadores mantenidos por LangChain:\n",
    "\n",
    "- Un adaptador en memoria, que usaremos en estos ejemplos.\n",
    "- Un adaptador para SQLite, adecuado para aplicaciones locales y pruebas.\n",
    "- Un adaptador para Postgres, optimizado para aplicaciones a gran escala.\n",
    "\n",
    "Muchos desarrolladores tambi√©n han creado adaptadores para otros sistemas como Redis o MySQL.\n",
    "\n",
    "Esto nos devuelve un objeto ejecutable con los mismos m√©todos que el utilizado anteriormente, pero ahora guarda el estado al final de cada paso. Esto significa que cada invocaci√≥n despu√©s de la primera ya no parte desde cero. Cada vez que se llama al grafo, comienza usando el *checkpointer* para recuperar el estado m√°s reciente guardado (si lo hay), y luego combina la nueva entrada con el estado anterior. Solo despu√©s de eso, se ejecutan los primeros nodos.\n",
    "\n",
    "Veamos la diferencia en acci√≥n.\n",
    "\n",
    "Observa el objeto `thread1`, que identifica la conversaci√≥n actual como perteneciente a un historial espec√≠fico de interacciones (lo que en LangGraph se conoce como *hilos* o *threads*). Los *threads* se crean autom√°ticamente la primera vez que se usan. Cualquier cadena puede ser un identificador v√°lido para un *thread* (com√∫nmente se usan UUIDs). La existencia de *threads* te permite lograr algo muy importante en una aplicaci√≥n con LLM: que m√∫ltiples usuarios puedan interactuar con el sistema al mismo tiempo, sin que sus conversaciones se mezclen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b55cd-a084-486f-936d-1651b16994c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Definimos el estado inicial del grafo\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Inicializamos el grafo con ese estado\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Modelo de lenguaje\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Nodo chatbot\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "# Agregamos el nodo al grafo\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Definimos conexiones (aristas)\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "# Compilamos el grafo con almacenamiento en memoria\n",
    "graph = builder.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8144777c-878f-48ee-ad26-3ac22da572a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos una conversaci√≥n identificada por un thread_id\n",
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5fc819-7e07-4bdd-a6b5-2ae70a690eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='b1c62c88-0530-44fe-a8e6-68193b7fa145'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNmwEioDZJmctgZOTEuZCJgmmKPYp', 'finish_reason': 'stop', 'logprobs': None}, id='run-437c6f8c-7445-4491-bc04-1b180731733d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "# Primera invocaci√≥n\n",
    "result_1 = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"hi, my name is Jack!\")]},\n",
    "    thread1\n",
    ")\n",
    "print(result_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4da7dd-b6ca-4c81-99a9-ece67bd0c06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='b1c62c88-0530-44fe-a8e6-68193b7fa145'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNmwEioDZJmctgZOTEuZCJgmmKPYp', 'finish_reason': 'stop', 'logprobs': None}, id='run-437c6f8c-7445-4491-bc04-1b180731733d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='6aef0c0e-9950-405a-9b9b-2266f2e9a5cf'), AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 37, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNmwH22WQKEeggFrLmqw8W34ZjmwQ', 'finish_reason': 'stop', 'logprobs': None}, id='run-e82abdcb-1d9a-4978-aa94-dcc856648482-0', usage_metadata={'input_tokens': 37, 'output_tokens': 6, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Segunda invocaci√≥n con memoria\n",
    "result_2 = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"what is my name?\")]},\n",
    "    thread1\n",
    ")\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d501a91-5dda-4d59-a53d-ffccea268715",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß† ¬øQu√© significa ‚Äúagregar memoria‚Äù a un grafo?\n",
    "\n",
    "Cuando t√∫ hablas con un chatbot, esperas que **recuerde lo que le dijiste antes**, ¬øcierto?\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "1. T√∫ dices: ‚ÄúHola, me llamo Jack.‚Äù\n",
    "2. Luego preguntas: ‚Äú¬øC√≥mo me llamo?‚Äù\n",
    "\n",
    "Un chatbot sin memoria **olvidar√≠a** lo que dijiste y no podr√≠a responderte bien.\n",
    "\n",
    "Pero con memoria, el sistema recuerda que t√∫ dijiste ‚Äúme llamo Jack‚Äù y puede responderte:  \n",
    "üëâ ‚ÄúTe llamas Jack.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ ¬øC√≥mo se logra eso en LangGraph?\n",
    "\n",
    "### Paso 1 ‚Äì Crear el grafo como siempre\n",
    "\n",
    "Armas tu grafo con tus nodos, reglas y conexiones. Hasta ah√≠ todo normal.\n",
    "\n",
    "### Paso 2 ‚Äì Cuando lo ‚Äúcompilas‚Äù, le agregas memoria\n",
    "\n",
    "Aqu√≠ es donde le dices:  \n",
    "> ‚ÄúPor favor, **guarda lo que va pasando**, para que no se le olvide lo anterior.‚Äù\n",
    "\n",
    "Eso se hace usando algo como `MemorySaver`, que es una forma simple de decirle:  \n",
    "üóíÔ∏è ‚ÄúGuarda todo en una memoria temporal (RAM) mientras el programa est√° corriendo.‚Äù\n",
    "\n",
    "Esto permite que el grafo recuerde paso a paso lo que el usuario ha dicho antes.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ ¬øY c√≥mo sabe qu√© conversaci√≥n es de qui√©n?\n",
    "\n",
    "Imagina que varias personas est√°n hablando con el mismo chatbot al mismo tiempo.\n",
    "\n",
    "Para que **no se mezclen las historias**, cada conversaci√≥n se etiqueta con un ID √∫nico, llamado `thread_id`.\n",
    "\n",
    "Es como tener varias conversaciones en WhatsApp: cada chat tiene su propio historial, aunque sea con la misma persona.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "{\"configurable\": {\"thread_id\": \"1\"}}\n",
    "```\n",
    "\n",
    "Este `thread_id = \"1\"` le dice al grafo:  \n",
    "üëâ ‚ÄúEsta conversaci√≥n es la de Jack.‚Äù\n",
    "\n",
    "Si despu√©s entra alguien m√°s (digamos Ana), se usar√≠a otro ID, como `\"2\"`, y as√≠ cada quien tiene su propio ‚Äúhilo de conversaci√≥n‚Äù separado.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Ejemplo contado como historia\n",
    "\n",
    "### Paso 1: Jack empieza a hablar\n",
    "\n",
    "El usuario (Jack) manda esto al grafo:\n",
    "\n",
    "```python\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"Hola, me llamo Jack.\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "El grafo responde algo como:  \n",
    "üëâ ‚Äú¬øC√≥mo puedo ayudarte, Jack?‚Äù\n",
    "\n",
    "Y **guarda internamente** que ‚ÄúJack se llama Jack‚Äù.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2: Jack hace otra pregunta\n",
    "\n",
    "M√°s tarde, Jack dice:\n",
    "\n",
    "```python\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"¬øC√≥mo me llamo?\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "Y el chatbot responde:\n",
    "\n",
    "üëâ ‚ÄúTe llamas Jack.‚Äù\n",
    "\n",
    "¬øPor qu√© lo sabe?  \n",
    "Porque est√° usando el mismo `thread_id = \"1\"` que se us√≥ antes, y **LangGraph recuerda lo que se dijo** en ese hilo.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† ¬øQu√© logras con esto?\n",
    "\n",
    "- Tu chatbot puede tener **conversaciones largas y coherentes**, sin tener que repetir todo cada vez.\n",
    "- Cada persona tiene su propio historial, gracias al `thread_id`.\n",
    "- Est√°s construyendo un agente que no solo responde‚Ä¶ sino que tambi√©n **recuerda**.\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b3dfd-52d0-4f7f-b511-1786d0f0fbcf",
   "metadata": {},
   "source": [
    "Mis disculpas por la confusi√≥n, aqu√≠ est√° la traducci√≥n del texto al principio y el c√≥digo al final, junto con la nota:\n",
    "\n",
    "---\n",
    "\n",
    "**Modificaci√≥n del Historial de Chat**\n",
    "\n",
    "En muchos casos, los mensajes del historial de chat no est√°n en el mejor estado o formato para generar una respuesta precisa del modelo. Para superar este problema, podemos modificar el historial de chat de tres maneras principales: recortando, filtrando y fusionando los mensajes.\n",
    "\n",
    "### Recorte de Mensajes\n",
    "\n",
    "Los **modelos de lenguaje (LLMs)** tienen ventanas de contexto limitadas; en otras palabras, hay un n√∫mero m√°ximo de tokens que los LLMs pueden recibir como entrada. Por lo tanto, el **mensaje final** enviado al modelo no debe exceder ese l√≠mite (particular para cada modo), ya que los modelos rechazar√°n un mensaje demasiado largo o lo truncar√°n. Adem√°s, la informaci√≥n excesiva en el mensaje puede distraer al modelo y llevar a **alucinaciones**.\n",
    "\n",
    "Una soluci√≥n efectiva a este problema es limitar el n√∫mero de mensajes que se recuperan del historial de chat y se agregan al mensaje. En la pr√°ctica, solo necesitamos cargar y almacenar los mensajes m√°s recientes.\n",
    "\n",
    "Afortunadamente, LangChain proporciona la ayuda incorporada **`trim_messages`** que incorpora varias estrategias para cumplir con estos requisitos. Por ejemplo, el **recortador** permite especificar cu√°ntos tokens queremos conservar o eliminar del historial de chat.\n",
    "\n",
    "Aqu√≠ hay un ejemplo que recupera los √∫ltimos **`max_tokens`** de la lista de mensajes configurando el par√°metro **`strategy`** a \"last\":\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**\n",
    "\n",
    "- El par√°metro **`strategy`** controla si se debe comenzar desde el principio o el final de la lista. Usualmente, querr√°s priorizar los mensajes m√°s recientes y recortar los m√°s antiguos si no caben. Es decir, empezar desde el final de la lista. Para este comportamiento, elige el valor **`last`**. La otra opci√≥n disponible es **`first`**, que prioriza los mensajes m√°s antiguos y recorta los m√°s recientes si no caben.\n",
    "  \n",
    "- El **`token_counter`** es un modelo LLM o de chat, que se utilizar√° para contar los tokens utilizando el tokenizador apropiado para ese modelo.\n",
    "\n",
    "- Podemos agregar el par√°metro **`include_system=True`** para asegurarnos de que el recortador mantenga el mensaje del sistema.\n",
    "\n",
    "- El par√°metro **`allow_partial`** determina si se debe recortar el contenido del √∫ltimo mensaje para ajustarse al l√≠mite. En nuestro ejemplo, lo configuramos en **`False`**, lo que elimina completamente el mensaje que exceder√≠a el l√≠mite total.\n",
    "\n",
    "- El par√°metro **`start_on=\"human\"`** asegura que nunca se eliminar√° un **`AIMessage`** (es decir, una respuesta del modelo) sin eliminar tambi√©n el mensaje correspondiente de **`HumanMessage`** (la pregunta para esa respuesta).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74757da5-3e7f-4ea9-88c8-df9e4e79977c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"what's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "# Configuraci√≥n del recorte de mensajes\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,               # L√≠mite de tokens\n",
    "    strategy=\"last\",             # Mantener los mensajes m√°s recientes\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),  # Contador de tokens para el modelo\n",
    "    include_system=True,         # Mantener el mensaje del sistema\n",
    "    allow_partial=False,         # No permitir cortar el √∫ltimo mensaje\n",
    "    start_on=\"human\"             # Empezar a cortar desde un mensaje de \"human\"\n",
    ")\n",
    "\n",
    "# Ejemplo de mensajes\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what's 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# Aplicar el recorte de mensajes\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58033f-3bd5-403b-872e-8471c287a090",
   "metadata": {},
   "source": [
    "**Filtrar Mensajes**\n",
    "\n",
    "A medida que la lista de mensajes del historial de chat crece, se pueden utilizar una mayor variedad de tipos, subcadenas y modelos. El helper **`filter_messages`** de LangChain facilita el filtrado de los mensajes del historial de chat por tipo, ID o nombre.\n",
    "\n",
    "---\n",
    "\n",
    "Aqu√≠ hay un ejemplo donde filtramos los mensajes humanos:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "\n",
    "filter_messages(messages, include_types=\"human\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Probemos otro ejemplo donde filtramos para excluir ciertos usuarios y IDs, e incluir tipos de mensajes:\n",
    "\n",
    "```python\n",
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "\n",
    "```python\n",
    "[SystemMessage(content='you are a good assistant', id='1'),\n",
    " HumanMessage(content='real input', name='bob', id='4'),\n",
    " AIMessage(content='real output', name='alice', id='5')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "filter_messages(\n",
    "    messages, \n",
    "    include_types=[HumanMessage, AIMessage], \n",
    "    exclude_ids=[\"3\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "\n",
    "```python\n",
    "[HumanMessage(content='example input', name='example_user', id='2'),\n",
    " HumanMessage(content='real input', name='bob', id='4'),\n",
    " AIMessage(content='real output', name='alice', id='5')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "El helper **`filter_messages`** tambi√©n puede usarse de forma imperativa o declarativa, lo que facilita su composici√≥n con otros componentes en una cadena:\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI()\n",
    "\n",
    "filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "\n",
    "chain = filter_ | model\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f371e-11b1-4d80-9a84-5951ec578b2a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß© Fusi√≥n de mensajes consecutivos (`merge_message_runs`)\n",
    "\n",
    "Algunos modelos de lenguaje ‚Äîcomo los modelos de Anthropic‚Äî **no permiten entradas que contengan m√∫ltiples mensajes consecutivos del mismo tipo** (por ejemplo, varios `SystemMessage` seguidos, o varios `AIMessage` juntos). Para resolver esto, LangChain proporciona una utilidad llamada `merge_message_runs`, que **fusiona de manera autom√°tica** esos mensajes repetidos, creando una secuencia m√°s limpia y compatible.\n",
    "\n",
    "---\n",
    "\n",
    "### üìã ¬øQu√© hace exactamente?\n",
    "\n",
    "`merge_message_runs` toma una lista de mensajes como:\n",
    "\n",
    "- `SystemMessage`\n",
    "- `HumanMessage`\n",
    "- `AIMessage`\n",
    "\n",
    "Y combina los que est√°n **uno seguido del otro y son del mismo tipo**, en un solo mensaje. El contenido de los mensajes se junta, ya sea como una cadena unificada (con saltos de l√≠nea) o como una lista de bloques de contenido (si alguno ya era una lista).\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Ejemplo:\n",
    "\n",
    "Sup√≥n que tienes este flujo de conversaci√≥n:\n",
    "\n",
    "- `SystemMessage(\"eres un buen asistente.\")`\n",
    "- `SystemMessage(\"siempre respondes con un chiste.\")`\n",
    "- `HumanMessage` con contenido tipo bloque (una lista con `type: text`)\n",
    "- Otro `HumanMessage` con texto plano\n",
    "- Dos `AIMessage` seguidos, cada uno con una broma\n",
    "\n",
    "Al usar `merge_message_runs`, se fusionan todos los mensajes del mismo tipo que est√°n seguidos, resultando en:\n",
    "\n",
    "- Un √∫nico `SystemMessage` con los dos textos unidos por `\\n`\n",
    "- Un √∫nico `HumanMessage` que une el bloque de contenido con el texto plano como lista\n",
    "- Un √∫nico `AIMessage` con las dos respuestas concatenadas con salto de l√≠nea\n",
    "\n",
    "---\n",
    "\n",
    "### üìé Resultado esperado\n",
    "\n",
    "```python\n",
    "[\n",
    "  SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\"),\n",
    "  HumanMessage(content=[\n",
    "      {'type': 'text', 'text': \"i wonder why it's called langchain\"},\n",
    "      'and who is harrison chasing anyway'\n",
    "  ]),\n",
    "  AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!')\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß∞ Composici√≥n con otros componentes\n",
    "\n",
    "Esta utilidad puede usarse de forma:\n",
    "\n",
    "- **Imperativa** (simplemente llam√°ndola sobre una lista de mensajes).\n",
    "- **Declarativa**, como parte de un `chain`, es decir, una cadena de pasos en LangChain.\n",
    "\n",
    "Esto te permite integrarla con modelos de lenguaje de forma fluida, por ejemplo:\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI()\n",
    "merger = merge_message_runs()\n",
    "chain = merger | model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üêç C√≥digo en Python (completo):\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    merge_message_runs,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    SystemMessage(\"you always respond with a joke.\"),\n",
    "    HumanMessage(\n",
    "        [{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyway\"),\n",
    "    AIMessage(\n",
    "        '''Well, I guess they thought \"WordRope\" and \"SentenceString\" just \n",
    "        didn't have the same ring to it!'''\n",
    "    ),\n",
    "    AIMessage(\"\"\"Why, he's probably chasing after the last cup of coffee in the \n",
    "        office!\"\"\"),\n",
    "]\n",
    "\n",
    "merged = merge_message_runs(messages)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae01a1c-8f99-4d45-973d-1601524c533f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
