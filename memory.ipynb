{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192e28a-725f-4b6a-a058-fc1e1a54165c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615fcfdd-0e51-4f38-8929-af874bdc2521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = key  # reemplaza con tu clave real si es necesario\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c364ee-eb4d-4af8-b535-c1fdc0b5a63b",
   "metadata": {},
   "source": [
    "\n",
    "## Usando LangGraph para Agregar Memoria a tu Chatbot\n",
    "\n",
    "En el Capítulo 3, aprendiste cómo proporcionar a tu aplicación de chatbot con IA un contexto actualizado y relevante. Esto le permite generar respuestas precisas basadas en lo que el usuario escribe. Pero eso no es suficiente para tener una aplicación lista para producción. ¿Cómo puedes hacer que tu chatbot realmente converse de ida y vuelta con el usuario, recordando lo que se ha dicho antes?\n",
    "\n",
    "Los modelos de lenguaje grandes (**LLMs**) son **sin estado**, lo que significa que cada vez que se les da un nuevo mensaje, no recuerdan el mensaje anterior ni sus propias respuestas. Para que puedan “recordar” conversaciones pasadas, necesitamos un sistema de memoria que lleve el seguimiento de los intercambios anteriores y del contexto relevante. Esta información histórica puede incluirse en el prompt final que se envía al modelo, dándole así una “memoria”.\n",
    "\n",
    "---\n",
    "\n",
    "## Construyendo un Sistema de Memoria para un Chatbot\n",
    "\n",
    "Hay dos decisiones clave que tomar al diseñar un sistema de memoria:\n",
    "\n",
    "1. **Cómo se almacena el estado**\n",
    "2. **Cómo se consulta ese estado**\n",
    "\n",
    "Una forma simple y efectiva de construir un sistema de memoria es guardar todo el historial de la conversación entre el usuario y el modelo, y reutilizarlo. El estado de este sistema puede:\n",
    "\n",
    "- Guardarse como una lista de mensajes (ver Capítulo 1 para más detalles sobre el formato)\n",
    "- Actualizarse agregando los mensajes más recientes después de cada turno\n",
    "- Insertarse en el prompt para que el modelo lo tenga en cuenta\n",
    "\n",
    "---\n",
    "\n",
    "## Ejemplo de implementación simple con LangChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f237dfaf-eaee-46ec-9950-2b7971969503",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acabo de decir en francés \"J'adore programmer\", que significa \"Me encanta programar\" en español.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Definimos el prompt que incluye un mensaje del sistema y un marcador para los mensajes anteriores\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente útil. Responde todas las preguntas lo mejor que puedas.\"\"\"),\n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Instanciamos el modelo de chat\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Encadenamos el prompt y el modelo\n",
    "chain = prompt | model\n",
    "\n",
    "# Simulamos una conversación previa\n",
    "respuesta = chain.invoke({\n",
    "    \"messages\": [\n",
    "        (\"human\", \"Traduce esta oración del inglés al francés: I love programming.\"),\n",
    "        (\"ai\", \"J'adore programmer.\"),\n",
    "        (\"human\", \"¿Qué acabas de decir?\"),\n",
    "    ],\n",
    "})\n",
    "\n",
    "# Mostramos la respuesta generada por el modelo\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491b3e5-bb71-4a21-a779-beb308863496",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Resultado esperado:\n",
    "\n",
    "```\n",
    "Dije: \"J'adore programmer\", que significa \"I love programming\" en francés.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Limitaciones y desafíos en producción\n",
    "\n",
    "Este enfoque simple funciona, pero en producción enfrentarás algunos retos adicionales:\n",
    "\n",
    "- Es necesario actualizar la memoria **después de cada interacción**, de forma **atómica** (registrar tanto la pregunta como la respuesta).\n",
    "- Se recomienda almacenar la memoria en un sistema duradero, como una base de datos relacional.\n",
    "- Se debe controlar **cuántos mensajes** se almacenan y cuántos se usan en cada nueva interacción.\n",
    "- Es útil poder **inspeccionar y modificar el estado** (la lista de mensajes) desde fuera del flujo de ejecución del modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d733c4-4fa3-4dfd-8e78-0721bbe4dd3e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introducción a LangGraph\n",
    "\n",
    "A partir de este capítulo y durante los siguientes, comenzaremos a utilizar **LangGraph**, una biblioteca de código abierto creada por **LangChain**. LangGraph está diseñada para permitir a los desarrolladores implementar **arquitecturas cognitivas con múltiples actores, múltiples pasos y con estado**, conocidas como *graphs* o **grafos**.\n",
    "\n",
    "Eso puede sonar como muchos conceptos en una sola oración, así que vamos a desglosarlo.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones con múltiples actores\n",
    "\n",
    "![Figura 4-3: De aplicaciones con un solo actor a aplicaciones con múltiples actores]\n",
    "\n",
    "Al igual que un equipo de especialistas puede lograr cosas que una sola persona no podría, las aplicaciones basadas en LLMs también pueden beneficiarse cuando se combinan con otras herramientas. Por ejemplo:\n",
    "\n",
    "- Un **prompt de LLM** es excelente para generación de texto o planeación de tareas.\n",
    "- Un **buscador web** es ideal para encontrar hechos actuales.\n",
    "\n",
    "Cuando combinas ambos —e incluso varios LLMs diferentes entre sí— puedes crear aplicaciones innovadoras como **Perplexity** o **Arc Search**, que combinan estos elementos para lograr algo mucho más potente.\n",
    "\n",
    "Así como los equipos humanos necesitan **coordinación**, una aplicación con múltiples actores también necesita una **capa de coordinación** para:\n",
    "\n",
    "- Definir los actores involucrados (los **nodos** del grafo) y cómo se comunican entre sí (las **aristas**).\n",
    "- Programar la ejecución de cada actor en el momento adecuado, incluso en **paralelo** y de forma **determinista**.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones con múltiples pasos\n",
    "\n",
    "![Figura 4-4: De múltiples actores a múltiples pasos]\n",
    "\n",
    "Cuando un actor entrega trabajo a otro (por ejemplo, un LLM que hace una pregunta a una herramienta de búsqueda), el sistema necesita saber:\n",
    "\n",
    "- En qué orden ocurren los eventos.\n",
    "- Cuántas veces se llama a cada actor.\n",
    "- Cuándo se detiene el proceso.\n",
    "\n",
    "Esto se puede modelar como una **secuencia de pasos discretos**. Cada paso representa una transferencia de trabajo, hasta que ya no quedan tareas pendientes y se llega a un resultado final.\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones con estado\n",
    "\n",
    "![Figura 4-5: De múltiples pasos a aplicaciones con estado]\n",
    "\n",
    "Para coordinar acciones entre pasos, se necesita **mantener un estado**. Si no se hace esto, el LLM daría la misma respuesta cada vez que se le llama. Al centralizar el estado:\n",
    "\n",
    "- Se puede **almacenar un snapshot** del estado en cualquier momento.\n",
    "- Se puede **pausar y reanudar** la ejecución, útil para recuperación de errores.\n",
    "- Se pueden **integrar humanos en el flujo de trabajo** (más sobre esto en el Capítulo 8).\n",
    "\n",
    "---\n",
    "\n",
    "## Componentes de un grafo en LangGraph\n",
    "\n",
    "Cada grafo en LangGraph tiene tres elementos clave:\n",
    "\n",
    "1. **Estado (State)**  \n",
    "   Datos que entran a la aplicación y se modifican mientras el grafo está en ejecución.\n",
    "\n",
    "2. **Nodos (Nodes)**  \n",
    "   Pasos individuales representados como funciones Python. Cada función recibe el estado actual, lo modifica (si es necesario) y devuelve una nueva versión del estado.\n",
    "\n",
    "3. **Aristas (Edges)**  \n",
    "   Conexiones entre los nodos. Estas pueden ser:\n",
    "   - **Fijas**: por ejemplo, después del nodo B siempre va el nodo D.\n",
    "   - **Condicionales**: se evalúa una función para decidir a qué nodo ir después.\n",
    "\n",
    "LangGraph también permite visualizar estos grafos y ofrece herramientas útiles para depuración y despliegue en producción a gran escala.\n",
    "\n",
    "---\n",
    "\n",
    "## Instalación de LangGraph\n",
    "\n",
    "Si seguiste las instrucciones del Capítulo 1, ya deberías tener LangGraph instalado. Si no, puedes instalarlo con:\n",
    "\n",
    "```bash\n",
    "pip install langgraph\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Primer ejemplo: Un chatbot simple con LangGraph\n",
    "\n",
    "Para familiarizarnos con LangGraph, construiremos un chatbot simple. Este responderá directamente a los mensajes del usuario utilizando un solo LLM. Aunque es un ejemplo sencillo, ilustra los **principios clave** de LangGraph:\n",
    "\n",
    "- Cómo se define un nodo.\n",
    "- Cómo se actualiza el estado.\n",
    "- Cómo fluye la información en el grafo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364555a-4d81-4f4e-9649-e27363dae5a1",
   "metadata": {},
   "source": [
    "Claro, Omar. Aquí tienes el contenido traducido, reorganizado y con **todo el código Python en una sola celda ejecutable** al final, como lo pediste:\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Creación de un `StateGraph` con LangGraph (Python)\n",
    "\n",
    "### 📋 Descripción paso a paso\n",
    "\n",
    "Para comenzar a usar **LangGraph**, lo primero que debes hacer es definir el **estado del grafo**. Este estado define:\n",
    "\n",
    "1. La estructura del estado compartido entre nodos (por ejemplo, un historial de mensajes).\n",
    "2. Cómo se actualiza cada parte del estado cuando un nodo devuelve un nuevo valor.\n",
    "3. En este ejemplo, usamos `add_messages`, que indica que los mensajes nuevos deben **agregarse** a la lista existente, en lugar de sobrescribirla.\n",
    "\n",
    "### 💬 Estado y Nodo\n",
    "\n",
    "Definimos una clase `State` que representa el estado compartido y un nodo llamado `chatbot`, que usa un modelo de lenguaje (`ChatOpenAI`) para generar una respuesta basada en los mensajes anteriores. El resultado se agrega nuevamente al estado gracias a la anotación `add_messages`.\n",
    "\n",
    "### 🧩 Conexión de nodos y compilación\n",
    "\n",
    "Luego, conectamos los nodos al punto de inicio (`START`) y final (`END`), y compilamos el grafo en un objeto ejecutable.\n",
    "\n",
    "### 📊 Visualización y ejecución\n",
    "\n",
    "Puedes visualizar el grafo generado con `draw_mermaid_png()` y ejecutarlo con `stream`, que irá devolviendo el estado tras cada paso del grafo.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Código Python completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "014298e1-7752-4003-af8c-384ef08aaf76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué proveedores ofrecen modelos LLM?\n",
      "LLM (Language Model) es un tipo de modelo de aprendizaje automático que se utiliza en el procesamiento del lenguaje natural. Algunos proveedores que ofrecen modelos LLM incluyen:\n",
      "\n",
      "1. OpenAI: Ofrece GPT-3, uno de los modelos de lenguaje más avanzados disponibles. \n",
      "\n",
      "2. Google: Ofrece BERT y T5, que son modelos de lenguaje que pueden ser utilizados para una variedad de tareas de procesamiento de lenguaje natural.\n",
      "\n",
      "3. Facebook AI: Ofrece RoBERTa, que es una variante de BERT que ha sido optimizada para tener un rendimiento aún mejor.\n",
      "\n",
      "4. Microsoft: Ofrece Turing, un modelo de lenguaje que ha sido entrenado en una gran cantidad de texto de internet.\n",
      "\n",
      "5. Hugging Face: Ofrece una variedad de modelos de lenguaje pre-entrenados que pueden ser utilizados para una variedad de tareas de procesamiento de lenguaje natural.\n",
      "\n",
      "6. IBM: Ofrece Watson, que incluye capacidades de procesamiento de lenguaje natural y puede ser utilizado para una variedad de tareas.\n",
      "\n",
      "7. Amazon: Ofrece Comprehend, un servicio que utiliza el aprendizaje automático para descubrir información en texto.\n",
      "\n",
      "8. Alibaba Cloud: Ofrece un modelo de lenguaje basado en la nube para el procesamiento del lenguaje natural. \n",
      "\n",
      "Estos son solo algunos ejemplos y hay muchos otros proveedores que ofrecen modelos de lenguaje.\n"
     ]
    }
   ],
   "source": [
    "# LangGraph básico con un solo nodo de ChatOpenAI (versión compatible)\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "# 1. Definir el esquema del estado (forma segura con TypedDict)\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "# 2. Instanciar modelo LLM\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0.3)\n",
    "\n",
    "# 3. Definir el nodo chatbot\n",
    "def chatbot(state: GraphState):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "# 4. Construir el grafo\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "builder.add_edge(\"chatbot\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. Ejecutar el grafo con mensaje inicial\n",
    "entrada = {\"messages\": [HumanMessage(content=\"¿Qué proveedores ofrecen modelos LLM?\")]}\n",
    "respuesta = graph.invoke(entrada)\n",
    "\n",
    "# 6. Mostrar respuesta del modelo\n",
    "for mensaje in respuesta[\"messages\"]:\n",
    "    print(mensaje.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bdff3-001f-4c99-98a5-6341ff6f45c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Agregar memoria a `StateGraph`\n",
    "\n",
    "LangGraph tiene persistencia integrada, que se utiliza de la misma forma tanto para los grafos más simples como para los más complejos. Veamos cómo se aplica esto a la arquitectura inicial.\n",
    "\n",
    "Vamos a recompilar nuestro grafo, pero ahora adjuntando un *checkpointer*, que es un adaptador de almacenamiento para LangGraph. LangGraph incluye una clase base que cualquier usuario puede extender para crear un adaptador para su base de datos favorita. Hasta el momento de esta escritura, LangGraph incluye varios adaptadores mantenidos por LangChain:\n",
    "\n",
    "- Un adaptador en memoria, que usaremos en estos ejemplos.\n",
    "- Un adaptador para SQLite, adecuado para aplicaciones locales y pruebas.\n",
    "- Un adaptador para Postgres, optimizado para aplicaciones a gran escala.\n",
    "\n",
    "Muchos desarrolladores también han creado adaptadores para otros sistemas como Redis o MySQL.\n",
    "\n",
    "Esto nos devuelve un objeto ejecutable con los mismos métodos que el utilizado anteriormente, pero ahora guarda el estado al final de cada paso. Esto significa que cada invocación después de la primera ya no parte desde cero. Cada vez que se llama al grafo, comienza usando el *checkpointer* para recuperar el estado más reciente guardado (si lo hay), y luego combina la nueva entrada con el estado anterior. Solo después de eso, se ejecutan los primeros nodos.\n",
    "\n",
    "Veamos la diferencia en acción.\n",
    "\n",
    "Observa el objeto `thread1`, que identifica la conversación actual como perteneciente a un historial específico de interacciones (lo que en LangGraph se conoce como *hilos* o *threads*). Los *threads* se crean automáticamente la primera vez que se usan. Cualquier cadena puede ser un identificador válido para un *thread* (comúnmente se usan UUIDs). La existencia de *threads* te permite lograr algo muy importante en una aplicación con LLM: que múltiples usuarios puedan interactuar con el sistema al mismo tiempo, sin que sus conversaciones se mezclen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220b55cd-a084-486f-936d-1651b16994c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated, TypedDict\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Definimos el estado inicial del grafo\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# Inicializamos el grafo con ese estado\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Modelo de lenguaje\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# Nodo chatbot\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "# Agregamos el nodo al grafo\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# Definimos conexiones (aristas)\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "# Compilamos el grafo con almacenamiento en memoria\n",
    "graph = builder.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8144777c-878f-48ee-ad26-3ac22da572a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creamos una conversación identificada por un thread_id\n",
    "thread1 = {\"configurable\": {\"thread_id\": \"1\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b5fc819-7e07-4bdd-a6b5-2ae70a690eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='b1c62c88-0530-44fe-a8e6-68193b7fa145'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNmwEioDZJmctgZOTEuZCJgmmKPYp', 'finish_reason': 'stop', 'logprobs': None}, id='run-437c6f8c-7445-4491-bc04-1b180731733d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "# Primera invocación\n",
    "result_1 = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"hi, my name is Jack!\")]},\n",
    "    thread1\n",
    ")\n",
    "print(result_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4da7dd-b6ca-4c81-99a9-ece67bd0c06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='hi, my name is Jack!', additional_kwargs={}, response_metadata={}, id='b1c62c88-0530-44fe-a8e6-68193b7fa145'), AIMessage(content='Hello Jack! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 14, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNmwEioDZJmctgZOTEuZCJgmmKPYp', 'finish_reason': 'stop', 'logprobs': None}, id='run-437c6f8c-7445-4491-bc04-1b180731733d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}, id='6aef0c0e-9950-405a-9b9b-2266f2e9a5cf'), AIMessage(content='Your name is Jack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 37, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNmwH22WQKEeggFrLmqw8W34ZjmwQ', 'finish_reason': 'stop', 'logprobs': None}, id='run-e82abdcb-1d9a-4978-aa94-dcc856648482-0', usage_metadata={'input_tokens': 37, 'output_tokens': 6, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Segunda invocación con memoria\n",
    "result_2 = graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"what is my name?\")]},\n",
    "    thread1\n",
    ")\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d501a91-5dda-4d59-a53d-ffccea268715",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 ¿Qué significa “agregar memoria” a un grafo?\n",
    "\n",
    "Cuando tú hablas con un chatbot, esperas que **recuerde lo que le dijiste antes**, ¿cierto?\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "1. Tú dices: “Hola, me llamo Jack.”\n",
    "2. Luego preguntas: “¿Cómo me llamo?”\n",
    "\n",
    "Un chatbot sin memoria **olvidaría** lo que dijiste y no podría responderte bien.\n",
    "\n",
    "Pero con memoria, el sistema recuerda que tú dijiste “me llamo Jack” y puede responderte:  \n",
    "👉 “Te llamas Jack.”\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 ¿Cómo se logra eso en LangGraph?\n",
    "\n",
    "### Paso 1 – Crear el grafo como siempre\n",
    "\n",
    "Armas tu grafo con tus nodos, reglas y conexiones. Hasta ahí todo normal.\n",
    "\n",
    "### Paso 2 – Cuando lo “compilas”, le agregas memoria\n",
    "\n",
    "Aquí es donde le dices:  \n",
    "> “Por favor, **guarda lo que va pasando**, para que no se le olvide lo anterior.”\n",
    "\n",
    "Eso se hace usando algo como `MemorySaver`, que es una forma simple de decirle:  \n",
    "🗒️ “Guarda todo en una memoria temporal (RAM) mientras el programa está corriendo.”\n",
    "\n",
    "Esto permite que el grafo recuerde paso a paso lo que el usuario ha dicho antes.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 ¿Y cómo sabe qué conversación es de quién?\n",
    "\n",
    "Imagina que varias personas están hablando con el mismo chatbot al mismo tiempo.\n",
    "\n",
    "Para que **no se mezclen las historias**, cada conversación se etiqueta con un ID único, llamado `thread_id`.\n",
    "\n",
    "Es como tener varias conversaciones en WhatsApp: cada chat tiene su propio historial, aunque sea con la misma persona.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```python\n",
    "{\"configurable\": {\"thread_id\": \"1\"}}\n",
    "```\n",
    "\n",
    "Este `thread_id = \"1\"` le dice al grafo:  \n",
    "👉 “Esta conversación es la de Jack.”\n",
    "\n",
    "Si después entra alguien más (digamos Ana), se usaría otro ID, como `\"2\"`, y así cada quien tiene su propio “hilo de conversación” separado.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Ejemplo contado como historia\n",
    "\n",
    "### Paso 1: Jack empieza a hablar\n",
    "\n",
    "El usuario (Jack) manda esto al grafo:\n",
    "\n",
    "```python\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"Hola, me llamo Jack.\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "El grafo responde algo como:  \n",
    "👉 “¿Cómo puedo ayudarte, Jack?”\n",
    "\n",
    "Y **guarda internamente** que “Jack se llama Jack”.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2: Jack hace otra pregunta\n",
    "\n",
    "Más tarde, Jack dice:\n",
    "\n",
    "```python\n",
    "graph.invoke(\n",
    "    {\"messages\": [HumanMessage(\"¿Cómo me llamo?\")]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "Y el chatbot responde:\n",
    "\n",
    "👉 “Te llamas Jack.”\n",
    "\n",
    "¿Por qué lo sabe?  \n",
    "Porque está usando el mismo `thread_id = \"1\"` que se usó antes, y **LangGraph recuerda lo que se dijo** en ese hilo.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 ¿Qué logras con esto?\n",
    "\n",
    "- Tu chatbot puede tener **conversaciones largas y coherentes**, sin tener que repetir todo cada vez.\n",
    "- Cada persona tiene su propio historial, gracias al `thread_id`.\n",
    "- Estás construyendo un agente que no solo responde… sino que también **recuerda**.\n",
    "\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b3dfd-52d0-4f7f-b511-1786d0f0fbcf",
   "metadata": {},
   "source": [
    "Mis disculpas por la confusión, aquí está la traducción del texto al principio y el código al final, junto con la nota:\n",
    "\n",
    "---\n",
    "\n",
    "**Modificación del Historial de Chat**\n",
    "\n",
    "En muchos casos, los mensajes del historial de chat no están en el mejor estado o formato para generar una respuesta precisa del modelo. Para superar este problema, podemos modificar el historial de chat de tres maneras principales: recortando, filtrando y fusionando los mensajes.\n",
    "\n",
    "### Recorte de Mensajes\n",
    "\n",
    "Los **modelos de lenguaje (LLMs)** tienen ventanas de contexto limitadas; en otras palabras, hay un número máximo de tokens que los LLMs pueden recibir como entrada. Por lo tanto, el **mensaje final** enviado al modelo no debe exceder ese límite (particular para cada modo), ya que los modelos rechazarán un mensaje demasiado largo o lo truncarán. Además, la información excesiva en el mensaje puede distraer al modelo y llevar a **alucinaciones**.\n",
    "\n",
    "Una solución efectiva a este problema es limitar el número de mensajes que se recuperan del historial de chat y se agregan al mensaje. En la práctica, solo necesitamos cargar y almacenar los mensajes más recientes.\n",
    "\n",
    "Afortunadamente, LangChain proporciona la ayuda incorporada **`trim_messages`** que incorpora varias estrategias para cumplir con estos requisitos. Por ejemplo, el **recortador** permite especificar cuántos tokens queremos conservar o eliminar del historial de chat.\n",
    "\n",
    "Aquí hay un ejemplo que recupera los últimos **`max_tokens`** de la lista de mensajes configurando el parámetro **`strategy`** a \"last\":\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:**\n",
    "\n",
    "- El parámetro **`strategy`** controla si se debe comenzar desde el principio o el final de la lista. Usualmente, querrás priorizar los mensajes más recientes y recortar los más antiguos si no caben. Es decir, empezar desde el final de la lista. Para este comportamiento, elige el valor **`last`**. La otra opción disponible es **`first`**, que prioriza los mensajes más antiguos y recorta los más recientes si no caben.\n",
    "  \n",
    "- El **`token_counter`** es un modelo LLM o de chat, que se utilizará para contar los tokens utilizando el tokenizador apropiado para ese modelo.\n",
    "\n",
    "- Podemos agregar el parámetro **`include_system=True`** para asegurarnos de que el recortador mantenga el mensaje del sistema.\n",
    "\n",
    "- El parámetro **`allow_partial`** determina si se debe recortar el contenido del último mensaje para ajustarse al límite. En nuestro ejemplo, lo configuramos en **`False`**, lo que elimina completamente el mensaje que excedería el límite total.\n",
    "\n",
    "- El parámetro **`start_on=\"human\"`** asegura que nunca se eliminará un **`AIMessage`** (es decir, una respuesta del modelo) sin eliminar también el mensaje correspondiente de **`HumanMessage`** (la pregunta para esa respuesta).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74757da5-3e7f-4ea9-88c8-df9e4e79977c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"what's 2 + 2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\n",
    "# Configuración del recorte de mensajes\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,               # Límite de tokens\n",
    "    strategy=\"last\",             # Mantener los mensajes más recientes\n",
    "    token_counter=ChatOpenAI(model=\"gpt-4o\"),  # Contador de tokens para el modelo\n",
    "    include_system=True,         # Mantener el mensaje del sistema\n",
    "    allow_partial=False,         # No permitir cortar el último mensaje\n",
    "    start_on=\"human\"             # Empezar a cortar desde un mensaje de \"human\"\n",
    ")\n",
    "\n",
    "# Ejemplo de mensajes\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what's 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "# Aplicar el recorte de mensajes\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e58033f-3bd5-403b-872e-8471c287a090",
   "metadata": {},
   "source": [
    "**Filtrar Mensajes**\n",
    "\n",
    "A medida que la lista de mensajes del historial de chat crece, se pueden utilizar una mayor variedad de tipos, subcadenas y modelos. El helper **`filter_messages`** de LangChain facilita el filtrado de los mensajes del historial de chat por tipo, ID o nombre.\n",
    "\n",
    "---\n",
    "\n",
    "Aquí hay un ejemplo donde filtramos los mensajes humanos:\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    filter_messages,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you are a good assistant\", id=\"1\"),\n",
    "    HumanMessage(\"example input\", id=\"2\", name=\"example_user\"),\n",
    "    AIMessage(\"example output\", id=\"3\", name=\"example_assistant\"),\n",
    "    HumanMessage(\"real input\", id=\"4\", name=\"bob\"),\n",
    "    AIMessage(\"real output\", id=\"5\", name=\"alice\"),\n",
    "]\n",
    "\n",
    "filter_messages(messages, include_types=\"human\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Probemos otro ejemplo donde filtramos para excluir ciertos usuarios y IDs, e incluir tipos de mensajes:\n",
    "\n",
    "```python\n",
    "filter_messages(messages, exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "\n",
    "```python\n",
    "[SystemMessage(content='you are a good assistant', id='1'),\n",
    " HumanMessage(content='real input', name='bob', id='4'),\n",
    " AIMessage(content='real output', name='alice', id='5')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "filter_messages(\n",
    "    messages, \n",
    "    include_types=[HumanMessage, AIMessage], \n",
    "    exclude_ids=[\"3\"]\n",
    ")\n",
    "```\n",
    "\n",
    "**Salida esperada:**\n",
    "\n",
    "```python\n",
    "[HumanMessage(content='example input', name='example_user', id='2'),\n",
    " HumanMessage(content='real input', name='bob', id='4'),\n",
    " AIMessage(content='real output', name='alice', id='5')]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "El helper **`filter_messages`** también puede usarse de forma imperativa o declarativa, lo que facilita su composición con otros componentes en una cadena:\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI()\n",
    "\n",
    "filter_ = filter_messages(exclude_names=[\"example_user\", \"example_assistant\"])\n",
    "\n",
    "chain = filter_ | model\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4f371e-11b1-4d80-9a84-5951ec578b2a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧩 Fusión de mensajes consecutivos (`merge_message_runs`)\n",
    "\n",
    "Algunos modelos de lenguaje —como los modelos de Anthropic— **no permiten entradas que contengan múltiples mensajes consecutivos del mismo tipo** (por ejemplo, varios `SystemMessage` seguidos, o varios `AIMessage` juntos). Para resolver esto, LangChain proporciona una utilidad llamada `merge_message_runs`, que **fusiona de manera automática** esos mensajes repetidos, creando una secuencia más limpia y compatible.\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 ¿Qué hace exactamente?\n",
    "\n",
    "`merge_message_runs` toma una lista de mensajes como:\n",
    "\n",
    "- `SystemMessage`\n",
    "- `HumanMessage`\n",
    "- `AIMessage`\n",
    "\n",
    "Y combina los que están **uno seguido del otro y son del mismo tipo**, en un solo mensaje. El contenido de los mensajes se junta, ya sea como una cadena unificada (con saltos de línea) o como una lista de bloques de contenido (si alguno ya era una lista).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Ejemplo:\n",
    "\n",
    "Supón que tienes este flujo de conversación:\n",
    "\n",
    "- `SystemMessage(\"eres un buen asistente.\")`\n",
    "- `SystemMessage(\"siempre respondes con un chiste.\")`\n",
    "- `HumanMessage` con contenido tipo bloque (una lista con `type: text`)\n",
    "- Otro `HumanMessage` con texto plano\n",
    "- Dos `AIMessage` seguidos, cada uno con una broma\n",
    "\n",
    "Al usar `merge_message_runs`, se fusionan todos los mensajes del mismo tipo que están seguidos, resultando en:\n",
    "\n",
    "- Un único `SystemMessage` con los dos textos unidos por `\\n`\n",
    "- Un único `HumanMessage` que une el bloque de contenido con el texto plano como lista\n",
    "- Un único `AIMessage` con las dos respuestas concatenadas con salto de línea\n",
    "\n",
    "---\n",
    "\n",
    "### 📎 Resultado esperado\n",
    "\n",
    "```python\n",
    "[\n",
    "  SystemMessage(content=\"you're a good assistant.\\nyou always respond with a joke.\"),\n",
    "  HumanMessage(content=[\n",
    "      {'type': 'text', 'text': \"i wonder why it's called langchain\"},\n",
    "      'and who is harrison chasing anyway'\n",
    "  ]),\n",
    "  AIMessage(content='Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!\\nWhy, he\\'s probably chasing after the last cup of coffee in the office!')\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧰 Composición con otros componentes\n",
    "\n",
    "Esta utilidad puede usarse de forma:\n",
    "\n",
    "- **Imperativa** (simplemente llamándola sobre una lista de mensajes).\n",
    "- **Declarativa**, como parte de un `chain`, es decir, una cadena de pasos en LangChain.\n",
    "\n",
    "Esto te permite integrarla con modelos de lenguaje de forma fluida, por ejemplo:\n",
    "\n",
    "```python\n",
    "model = ChatOpenAI()\n",
    "merger = merge_message_runs()\n",
    "chain = merger | model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 Código en Python (completo):\n",
    "\n",
    "```python\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    merge_message_runs,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    SystemMessage(\"you always respond with a joke.\"),\n",
    "    HumanMessage(\n",
    "        [{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}]\n",
    "    ),\n",
    "    HumanMessage(\"and who is harrison chasing anyway\"),\n",
    "    AIMessage(\n",
    "        '''Well, I guess they thought \"WordRope\" and \"SentenceString\" just \n",
    "        didn't have the same ring to it!'''\n",
    "    ),\n",
    "    AIMessage(\"\"\"Why, he's probably chasing after the last cup of coffee in the \n",
    "        office!\"\"\"),\n",
    "]\n",
    "\n",
    "merged = merge_message_runs(messages)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae01a1c-8f99-4d45-973d-1601524c533f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
