{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3401dbc-a5d3-4490-9078-b58736bf2573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = key  # reemplaza con tu clave real si es necesario\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260173e1-6291-4ad6-88ca-b5f9f4dbdee5",
   "metadata": {},
   "source": [
    "\n",
    "## Arquitecturas Cognitivas con LangGraph\n",
    "\n",
    "Hasta ahora, hemos visto las características más comunes de las aplicaciones con modelos de lenguaje (LLM):\n",
    "\n",
    "- Técnicas de creación de prompts en el Prefacio y el Capítulo 1  \n",
    "- RAG en los Capítulos 2 y 3  \n",
    "- Memoria en el Capítulo 4  \n",
    "\n",
    "La siguiente pregunta debería ser: ¿cómo ensamblamos estas piezas en una aplicación coherente que logre el objetivo que nos propusimos? Para hacer un paralelo con el mundo de los ladrillos y el cemento, una piscina y una casa de un solo piso están construidas con los mismos materiales, pero obviamente cumplen propósitos muy diferentes. Lo que las hace adecuadas para sus distintos fines es el plan sobre cómo se combinan esos materiales —es decir, su arquitectura. Lo mismo ocurre al construir aplicaciones con LLM. Las decisiones más importantes que debes tomar tienen que ver con cómo ensamblar los diferentes componentes a tu disposición (como RAG, técnicas de prompting, memoria) en algo que cumpla tu propósito.\n",
    "\n",
    "Antes de ver arquitecturas específicas, veamos un ejemplo. Cualquier aplicación con LLM que construyas comenzará con un propósito: lo que la aplicación está diseñada para hacer. Supongamos que quieres construir un asistente de correo electrónico —una aplicación con LLM que lea tus correos antes que tú y tenga como objetivo reducir la cantidad de correos que necesitas revisar. La aplicación podría hacer esto archivando algunos correos irrelevantes, respondiendo directamente a otros, y marcando algunos como merecedores de tu atención más adelante.\n",
    "\n",
    "Probablemente también querrías que la aplicación esté limitada por ciertas restricciones en sus acciones. Enumerar esas restricciones ayuda muchísimo, ya que te ayudarán a definir la arquitectura adecuada. El Capítulo 8 aborda estas restricciones con más detalle y cómo trabajar con ellas. Para este asistente de correo electrónico hipotético, digamos que nos gustaría que hiciera lo siguiente:\n",
    "\n",
    "- Minimizar la cantidad de veces que te interrumpe (después de todo, la idea es ahorrar tiempo).  \n",
    "- Evitar que tus corresponsales reciban una respuesta que tú nunca habrías enviado.  \n",
    "\n",
    "Esto apunta a un conflicto clave al construir aplicaciones con LLM: el equilibrio entre **agencia** (la capacidad de actuar de forma autónoma) y **confiabilidad** (el grado en que puedes confiar en sus respuestas). Intuitivamente, el asistente será más útil si actúa más sin tu intervención, pero si se le da demasiada libertad, inevitablemente enviará mensajes que preferirías no haber enviado.\n",
    "\n",
    "Una forma de describir el grado de autonomía de una aplicación con LLM es evaluar cuánta parte del comportamiento de la aplicación está determinada por un LLM (en vez de por código):\n",
    "\n",
    "- Hacer que un LLM decida el resultado de un paso (por ejemplo, redactar una respuesta a un correo).  \n",
    "- Hacer que un LLM decida cuál es el siguiente paso a tomar (por ejemplo, ante un nuevo correo, decidir entre archivar, responder o marcar para revisión).  \n",
    "- Hacer que un LLM decida qué pasos están disponibles para tomar (por ejemplo, que el LLM escriba código que ejecute una acción dinámica que no fue preprogramada en la aplicación).  \n",
    "\n",
    "Podemos clasificar varias recetas populares para construir aplicaciones con LLM en función de dónde caen en este espectro de autonomía —es decir, qué tareas de las tres anteriores son manejadas por un LLM y cuáles siguen en manos del desarrollador o del usuario. Estas recetas se pueden llamar **arquitecturas cognitivas**. En el campo de la inteligencia artificial, el término arquitectura cognitiva se ha usado durante mucho tiempo para denotar modelos de razonamiento humano (y sus implementaciones en computadoras). Una **arquitectura cognitiva con LLM** (el término fue aplicado a los LLM por primera vez, hasta donde sabemos, en un artículo¹) se puede definir como una receta de los pasos que debe seguir una aplicación con LLM (ver Figura 5-1). Un paso es, por ejemplo, recuperar documentos relevantes (RAG), o llamar a un LLM con un prompt de cadena de pensamiento (*chain-of-thought*).\n",
    "\n",
    "**Figura 5-1. Arquitecturas cognitivas para aplicaciones con LLM**\n",
    "\n",
    "Ahora veamos cada una de las principales arquitecturas, o recetas, que puedes usar al construir tu aplicación (como se muestra en la Figura 5-1):\n",
    "\n",
    "### 0: Código\n",
    "Esto no es una arquitectura cognitiva con LLM (por eso la numeramos como 0), ya que no utiliza LLMs en absoluto. Puedes pensar en esto como el software tradicional que estás acostumbrado a escribir. La primera arquitectura interesante (para este libro, al menos) es la siguiente.\n",
    "\n",
    "### 1: Llamada a LLM\n",
    "Esta es la mayoría de los ejemplos que hemos visto en el libro hasta ahora, con una sola llamada a un LLM. Es útil principalmente cuando forma parte de una aplicación más grande que usa un LLM para lograr una tarea específica, como traducir o resumir un texto.\n",
    "\n",
    "### 2: Cadena\n",
    "El siguiente nivel consiste en el uso de múltiples llamadas a LLM en una secuencia predefinida. Por ejemplo, una aplicación de texto a SQL (que recibe como entrada una descripción en lenguaje natural de algún cálculo a realizar sobre una base de datos) podría usar dos llamadas a LLM en secuencia:\n",
    "\n",
    "- Una llamada a LLM para generar una consulta SQL, a partir de la consulta en lenguaje natural proporcionada por el usuario y una descripción del contenido de la base de datos proporcionada por el desarrollador.  \n",
    "- Otra llamada a LLM para escribir una explicación de la consulta apropiada para un usuario no técnico, dada la consulta generada en la llamada anterior. Esta se puede usar para que el usuario verifique si la consulta generada coincide con su solicitud.\n",
    "\n",
    "### 3: Enrutador\n",
    "Este siguiente paso consiste en usar el LLM para definir la secuencia de pasos a seguir. Es decir, mientras que la arquitectura en cadena siempre ejecuta una secuencia estática de pasos (por muchos que sean) determinada por el desarrollador, la arquitectura de enrutador se caracteriza por usar un LLM para elegir entre ciertos pasos predefinidos. Un ejemplo sería una aplicación RAG con múltiples índices de documentos de diferentes dominios, con los siguientes pasos:\n",
    "\n",
    "- Una llamada a LLM para elegir cuál de los índices disponibles usar, dado la consulta proporcionada por el usuario y la descripción de los índices proporcionada por el desarrollador.  \n",
    "- Un paso de recuperación que consulta el índice elegido para obtener los documentos más relevantes para la consulta del usuario.  \n",
    "- Otra llamada a LLM para generar una respuesta, dada la consulta del usuario y la lista de documentos relevantes recuperados del índice.  \n",
    "\n",
    "Hasta aquí llega este capítulo. Hablaremos de cada una de estas arquitecturas más adelante. Los próximos capítulos discuten las **arquitecturas agenticas**, que hacen un uso aún mayor de los LLMs. Pero primero, hablemos de mejores herramientas para ayudarnos en este camino.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded51325-688e-441a-9d02-56e381c7a7b2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Arquitectura #1: Llamada a LLM\n",
    "\n",
    "Como ejemplo de la arquitectura basada en una llamada a un LLM, volveremos al chatbot que creamos en el Capítulo 4. Este chatbot responderá directamente a los mensajes del usuario.\n",
    "\n",
    "Comienza creando un `StateGraph`, al cual le agregaremos un nodo para representar la llamada al LLM:\n",
    "\n",
    "También podemos dibujar una representación visual del grafo:\n",
    "\n",
    "El grafo que acabamos de crear se ve como en la Figura 5-2.\n",
    "\n",
    "Puedes ejecutarlo con el método `stream()` que ya viste en capítulos anteriores:\n",
    "\n",
    "La salida:\n",
    "\n",
    "```json\n",
    "{ \"chatbot\": { \"messages\": [AIMessage(\"¿En qué puedo ayudarte?\")] } }\n",
    "```\n",
    "\n",
    "Observa cómo la entrada al grafo tiene el mismo formato que el objeto `State` que definimos anteriormente; es decir, enviamos una lista de mensajes dentro de la clave `messages` de un diccionario.\n",
    "\n",
    "Esta es la arquitectura más simple posible para utilizar un modelo de lenguaje (LLM), lo cual no significa que no deba usarse. Aquí algunos ejemplos de dónde podrías verla en acción en productos populares:\n",
    "\n",
    "- Funciones impulsadas por IA como **resumir** o **traducir** (como las que puedes encontrar en Notion, un software de escritura popular) pueden estar impulsadas por una sola llamada a un LLM.\n",
    "- La generación simple de consultas SQL también puede ser alimentada por una sola llamada a un LLM, dependiendo de la experiencia de usuario y el tipo de usuario objetivo que tenga en mente el desarrollador.\n",
    "\n",
    "---\n",
    "\n",
    "### Solo código en Python:\n",
    "\n",
    "```python\n",
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Los mensajes tienen el tipo \"lista\". La función `add_messages` \n",
    "    # en la anotación define cómo debe actualizarse este estado \n",
    "    # (en este caso, añade nuevos mensajes a la lista en lugar de reemplazarlos)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State):\n",
    "    answer = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "builder.add_edge(START, 'chatbot')\n",
    "builder.add_edge('chatbot', END)\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "# Dibujar representación del grafo\n",
    "graph.get_graph().draw_mermaid_png()\n",
    "\n",
    "# Ejecutar usando el método stream\n",
    "input = {\"messages\": [HumanMessage('hi!')]}\n",
    "for chunk in graph.stream(input):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cce43e-db63-4922-8f6f-2848b40117de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef595f-e4a0-4350-a0b9-b65be1dc808a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf84e14f-21a4-44d2-bacd-297c4b88609e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Arquitectura #2: Cadena (Chain)\n",
    "\n",
    "Esta siguiente arquitectura amplía la anterior usando múltiples llamadas a LLM, en una secuencia predefinida (es decir, distintas ejecuciones de la aplicación siguen la misma secuencia de llamadas LLM, aunque con diferentes entradas y resultados).\n",
    "\n",
    "Tomemos como ejemplo una aplicación de texto a SQL, que recibe como entrada del usuario una descripción en lenguaje natural de algún cálculo que quiere realizar sobre una base de datos. Mencionamos antes que esto podría lograrse con una sola llamada a un LLM, para generar una consulta SQL, pero podemos crear una aplicación más sofisticada usando múltiples llamadas LLM en secuencia. Algunos autores llaman a esta arquitectura *ingeniería de flujos*.\n",
    "\n",
    "Primero describamos el flujo con palabras:\n",
    "\n",
    "- Una llamada a LLM para generar una consulta SQL a partir de la pregunta en lenguaje natural proporcionada por el usuario, y una descripción del contenido de la base de datos proporcionada por el desarrollador.\n",
    "- Otra llamada a LLM para escribir una explicación de la consulta, adecuada para un usuario no técnico, con base en la consulta generada en la llamada anterior. Esto puede usarse para permitir que el usuario verifique si la consulta generada coincide con su solicitud.\n",
    "\n",
    "También podrías extender esto aún más (aunque no lo haremos aquí) con pasos adicionales después de los dos anteriores:\n",
    "\n",
    "- Ejecutar la consulta contra la base de datos, lo cual devuelve una tabla bidimensional.\n",
    "- Usar una tercera llamada a LLM para resumir los resultados en una respuesta textual a la pregunta original del usuario.\n",
    "\n",
    "Y ahora, implementemos esto con **LangGraph**:\n",
    "\n",
    "---\n",
    "\n",
    "### Código en Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfaa59b-0cef-4ab2-bcdf-d63987987903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Modelo con baja temperatura para generar consultas SQL\n",
    "model_low_temp = ChatOpenAI(temperature=0.1)\n",
    "# Modelo con mayor temperatura para generar explicaciones en lenguaje natural\n",
    "model_high_temp = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Historial de conversación\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # Entrada del usuario\n",
    "    user_query: str\n",
    "    # Salidas\n",
    "    sql_query: str\n",
    "    sql_explanation: str\n",
    "\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "\n",
    "class Output(TypedDict):\n",
    "    sql_query: str\n",
    "    sql_explanation: str\n",
    "\n",
    "generate_prompt = SystemMessage(\n",
    "    \"\"\"Eres un analista de datos útil que genera consultas SQL para los usuarios \n",
    "    en función de sus preguntas.\"\"\"\n",
    ")\n",
    "\n",
    "def generate_sql(state: State) -> State:\n",
    "    user_message = HumanMessage(state[\"user_query\"])\n",
    "    messages = [generate_prompt, *state[\"messages\"], user_message]\n",
    "    res = model_low_temp.invoke(messages)\n",
    "    return {\n",
    "        \"sql_query\": res.content,\n",
    "        \"messages\": [user_message, res],\n",
    "    }\n",
    "\n",
    "explain_prompt = SystemMessage(\n",
    "    \"Eres un analista de datos útil que explica consultas SQL a los usuarios.\"\n",
    ")\n",
    "\n",
    "def explain_sql(state: State) -> State:\n",
    "    messages = [\n",
    "        explain_prompt,\n",
    "        *state[\"messages\"],\n",
    "    ]\n",
    "    res = model_high_temp.invoke(messages)\n",
    "    return {\n",
    "        \"sql_explanation\": res.content,\n",
    "        \"messages\": res,\n",
    "    }\n",
    "\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"generate_sql\", generate_sql)\n",
    "builder.add_node(\"explain_sql\", explain_sql)\n",
    "builder.add_edge(START, \"generate_sql\")\n",
    "builder.add_edge(\"generate_sql\", \"explain_sql\")\n",
    "builder.add_edge(\"explain_sql\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c389525a-08f2-4149-8766-972a0f02a2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_query': 'Para obtener el total de ventas de cada producto, puedes utilizar la siguiente consulta SQL:\\n\\n```sql\\nSELECT product_name, SUM(sales_amount) AS total_sales\\nFROM sales\\nGROUP BY product_name;\\n```\\n\\nEsta consulta te mostrará el nombre de cada producto junto con el total de ventas de ese producto. Asegúrate de reemplazar \"product_name\" y \"sales_amount\" con los nombres reales de las columnas en tu base de datos.',\n",
       " 'sql_explanation': '¿Te gustaría saber algo más sobre consultas SQL?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\n",
    "  \"user_query\": \"What is the total sales for each product?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695282f-9c48-4ab2-8709-cbcf12ce892c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e37d5f0-8738-4200-98fb-e5b37e2aea45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Arquitectura #3: Enrutador (Router)\n",
    "\n",
    "Esta siguiente arquitectura sube en la escala de autonomía al asignar a los LLMs la siguiente responsabilidad que mencionamos antes: decidir el siguiente paso a tomar. Es decir, mientras que la arquitectura de cadena siempre ejecuta una secuencia estática de pasos (cuantos más pasos, mejor), la arquitectura de enrutador se caracteriza por utilizar un LLM para elegir entre ciertos pasos predefinidos.\n",
    "\n",
    "Tomemos como ejemplo una aplicación **RAG** con acceso a múltiples índices de documentos de diferentes dominios (consulta el Capítulo 2 para más detalles sobre indexación). Usualmente, puedes obtener un mejor rendimiento de los LLMs evitando la inclusión de información irrelevante en el prompt. Por lo tanto, al construir esta aplicación, debemos intentar seleccionar el índice adecuado para cada consulta y usar solo ese.\n",
    "\n",
    "El desarrollo clave en esta arquitectura es utilizar un LLM para tomar esta decisión, utilizando efectivamente un LLM para evaluar cada consulta entrante y decidir qué índice usar para esa consulta en particular.\n",
    "\n",
    "**NOTA**  \n",
    "Antes de la llegada de los LLMs, la forma habitual de resolver este problema sería construir un modelo de clasificación utilizando técnicas de ML y un conjunto de datos que mapea las consultas de los usuarios al índice correcto. Esto podría resultar bastante desafiante, ya que requiere lo siguiente:\n",
    "\n",
    "- Armar ese conjunto de datos manualmente.\n",
    "- Generar suficientes características (atributos cuantitativos) de cada consulta de usuario para habilitar el entrenamiento de un clasificador para la tarea.\n",
    "\n",
    "Los LLMs, dado su codificado del lenguaje humano, pueden servir efectivamente como este clasificador con cero o muy pocos ejemplos o entrenamiento adicional.\n",
    "\n",
    "Primero, describamos el flujo en palabras:\n",
    "\n",
    "1. Una llamada a LLM para elegir qué de los índices disponibles usar, dada la consulta proporcionada por el usuario, y la descripción del índice proporcionada por el desarrollador.\n",
    "2. Un paso de recuperación que consulta el índice elegido para los documentos más relevantes para la consulta del usuario.\n",
    "3. Otra llamada a LLM para generar una respuesta, dada la consulta del usuario y la lista de documentos relevantes obtenidos del índice.\n",
    "\n",
    "Y ahora, implementémoslo con **LangGraph**:\n",
    "\n",
    "---\n",
    "\n",
    "### Solo código en Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd007b9-ae16-42ec-9970-bf0ad3537804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.vectorstores.in_memory import InMemoryVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# útil para generar consultas SQL\n",
    "model_low_temp = ChatOpenAI(temperature=0.1)\n",
    "# útil para generar salidas en lenguaje natural\n",
    "model_high_temp = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "class State(TypedDict):\n",
    "    # para rastrear el historial de conversación\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # entrada\n",
    "    user_query: str\n",
    "    # salida\n",
    "    domain: Literal[\"records\", \"insurance\"]\n",
    "    documents: list[Document]\n",
    "    answer: str\n",
    "\n",
    "class Input(TypedDict):\n",
    "    user_query: str\n",
    "\n",
    "class Output(TypedDict):\n",
    "    documents: list[Document]\n",
    "    answer: str\n",
    "\n",
    "# referencia al Capítulo 2 sobre cómo llenar un vector store con documentos\n",
    "medical_records_store = InMemoryVectorStore.from_documents([], embeddings)\n",
    "medical_records_retriever = medical_records_store.as_retriever()\n",
    "\n",
    "insurance_faqs_store = InMemoryVectorStore.from_documents([], embeddings)\n",
    "insurance_faqs_retriever = insurance_faqs_store.as_retriever()\n",
    "\n",
    "router_prompt = SystemMessage(\n",
    "    \"\"\"Debes decidir a qué dominio enrutar la consulta del usuario. Tienes dos \n",
    "        dominios para elegir:\n",
    "          - records: contiene los registros médicos del paciente, como \n",
    "          diagnóstico, tratamiento y prescripciones.\n",
    "          - insurance: contiene preguntas frecuentes sobre políticas de \n",
    "          seguros, reclamaciones y cobertura.\n",
    "\n",
    "Solo devuelve el nombre del dominio.\"\"\"\n",
    ")\n",
    "\n",
    "def router_node(state: State) -> State:\n",
    "    user_message = HumanMessage(state[\"user_query\"])\n",
    "    messages = [router_prompt, *state[\"messages\"], user_message]\n",
    "    res = model_low_temp.invoke(messages)\n",
    "    return {\n",
    "        \"domain\": res.content,\n",
    "        \"messages\": [user_message, res],\n",
    "    }\n",
    "\n",
    "def pick_retriever(\n",
    "    state: State,\n",
    ") -> Literal[\"retrieve_medical_records\", \"retrieve_insurance_faqs\"]:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        return \"retrieve_medical_records\"\n",
    "    else:\n",
    "        return \"retrieve_insurance_faqs\"\n",
    "\n",
    "def retrieve_medical_records(state: State) -> State:\n",
    "    documents = medical_records_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "\n",
    "def retrieve_insurance_faqs(state: State) -> State:\n",
    "    documents = insurance_faqs_retriever.invoke(state[\"user_query\"])\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "    }\n",
    "\n",
    "medical_records_prompt = SystemMessage(\n",
    "    \"\"\"Eres un chatbot médico útil que responde preguntas basadas en los \n",
    "        registros médicos del paciente, como diagnóstico, tratamiento y \n",
    "        prescripciones.\"\"\"\n",
    ")\n",
    "\n",
    "insurance_faqs_prompt = SystemMessage(\n",
    "    \"\"\"Eres un chatbot de seguros útil que responde preguntas frecuentes sobre \n",
    "        pólizas de seguros, reclamaciones y cobertura.\"\"\"\n",
    ")\n",
    "\n",
    "def generate_answer(state: State) -> State:\n",
    "    if state[\"domain\"] == \"records\":\n",
    "        prompt = medical_records_prompt\n",
    "    else:\n",
    "        prompt = insurance_faqs_prompt\n",
    "    messages = [\n",
    "        prompt,\n",
    "        *state[\"messages\"],\n",
    "        HumanMessage(f\"Documentos: {state['documents']}\"),\n",
    "    ]\n",
    "    res = model_high_temp.invoke(messages)\n",
    "    return {\n",
    "        \"answer\": res.content,\n",
    "        \"messages\": res,\n",
    "    }\n",
    "\n",
    "builder = StateGraph(State, input=Input, output=Output)\n",
    "builder.add_node(\"router\", router_node)\n",
    "builder.add_node(\"retrieve_medical_records\", retrieve_medical_records)\n",
    "builder.add_node(\"retrieve_insurance_faqs\", retrieve_insurance_faqs)\n",
    "builder.add_node(\"generate_answer\", generate_answer)\n",
    "builder.add_edge(START, \"router\")\n",
    "builder.add_conditional_edges(\"router\", pick_retriever)\n",
    "builder.add_edge(\"retrieve_medical_records\", \"generate_answer\")\n",
    "builder.add_edge(\"retrieve_insurance_faqs\", \"generate_answer\")\n",
    "builder.add_edge(\"generate_answer\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6d3cab-a3cd-4466-ac09-5eac8be90a3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'router': {'domain': 'insurance', 'messages': [HumanMessage(content='Am I covered for COVID-19 treatment?', additional_kwargs={}, response_metadata={}, id='36e25d15-1a8b-4381-91aa-b248d8f4402a'), AIMessage(content='insurance', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 104, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNoMliA7ydCVGVC1nfnbodxsfbv7Z', 'finish_reason': 'stop', 'logprobs': None}, id='run-69c0b1dc-2ef1-4aee-9a08-831590befe2a-0', usage_metadata={'input_tokens': 104, 'output_tokens': 2, 'total_tokens': 106, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "{'retrieve_insurance_faqs': {'documents': []}}\n",
      "{'generate_answer': {'answer': 'Para determinar si estás cubierto para el tratamiento de COVID-19, necesitaría más información, como el tipo de póliza de seguro que tienes y las condiciones específicas de tu cobertura. Te recomendaría que revises tu póliza de seguro o te pongas en contacto con tu compañía de seguros para obtener información detallada sobre la cobertura de tratamiento para COVID-19. ¿Hay algo más en lo que pueda ayudarte?', 'messages': AIMessage(content='Para determinar si estás cubierto para el tratamiento de COVID-19, necesitaría más información, como el tipo de póliza de seguro que tienes y las condiciones específicas de tu cobertura. Te recomendaría que revises tu póliza de seguro o te pongas en contacto con tu compañía de seguros para obtener información detallada sobre la cobertura de tratamiento para COVID-19. ¿Hay algo más en lo que pueda ayudarte?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 68, 'total_tokens': 167, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BNoMnfnbm4VFAEg83WHozbmOBB65q', 'finish_reason': 'stop', 'logprobs': None}, id='run-ff79169e-8ef4-47d6-9e14-1654c644d179-0', usage_metadata={'input_tokens': 68, 'output_tokens': 99, 'total_tokens': 167, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}}\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "    \"user_query\": \"Am I covered for COVID-19 treatment?\"\n",
    "}\n",
    "for c in graph.stream(input):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae23b7-b2b7-4c4e-8a90-8e2859ed6731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
